{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ed2c218a86749bdb0c9c0ac9712950bfa3b9bf9e"
   },
   "source": [
    "# Complete optimisation of a LightGBM model using random search\n",
    "Features that are illustrated in this kernel:\n",
    "- a bit of data cleaning following https://www.kaggle.com/mlisovyi/categorical-variables-in-the-data and https://www.kaggle.com/mlisovyi/missing-values-in-the-data\n",
    "- **gradient-boosted decision trees** using _**LightGBM**_ package\n",
    "- **early stopping** in _**LightGBM**_ model training using **F1 macro score** to avoid overfotting\n",
    "- **learning rate decay** in _**LightGBM**_ model training to improve convergence to the minimum\n",
    "- **hyperparameter optimisation** of the model using random search in cross validation with F1 macro score\n",
    "- submission preparation\n",
    "This kernel inherited ideas and SW solutions from other public kernels and in such cases I will post direct references to the original product, that that you can get some additional insights from the source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8e4e08a17549fd247619178c96c3ade2519e9773"
   },
   "source": [
    "The following categorical mapping originates from [this kernel](https://www.kaggle.com/mlisovyi/categorical-variables-encoding-function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def encode_data(df):\n",
    "    '''\n",
    "    The function does not return, but transforms the input pd.DataFrame\n",
    "    \n",
    "    Encodes the Costa Rican Household Poverty Level data \n",
    "    following studies in https://www.kaggle.com/mlisovyi/categorical-variables-in-the-data\n",
    "    and the insight from https://www.kaggle.com/c/costa-rican-household-poverty-prediction/discussion/61403#359631\n",
    "    \n",
    "    The following columns get transformed: edjefe, edjefa, dependency, idhogar\n",
    "    The user most likely will simply drop idhogar completely (after calculating houshold-level aggregates)\n",
    "    '''\n",
    "    \n",
    "    yes_no_map = {'no': 0, 'yes': 1}\n",
    "    \n",
    "    df['dependency'] = df['dependency'].replace(yes_no_map).astype(np.float32)\n",
    "    \n",
    "    df['edjefe'] = df['edjefe'].replace(yes_no_map).astype(np.float32)\n",
    "    df['edjefa'] = df['edjefa'].replace(yes_no_map).astype(np.float32)\n",
    "    \n",
    "    df['idhogar'] = LabelEncoder().fit_transform(df['idhogar'])\n",
    "\n",
    "    \n",
    "def do_features(df):\n",
    "    feats_div = [('children_fraction', 'r4t1', 'r4t3'), \n",
    "                 ('working_man_fraction', 'r4h2', 'r4t3'),\n",
    "                 ('all_man_fraction', 'r4h3', 'r4t3'),\n",
    "                 ('human_density', 'tamviv', 'rooms'),\n",
    "                 ('human_bed_density', 'tamviv', 'bedrooms'),\n",
    "                 ('rent_per_person', 'v2a1', 'r4t3'),\n",
    "                 ('rent_per_room', 'v2a1', 'rooms'),\n",
    "                 ('mobile_density', 'qmobilephone', 'r4t3'),\n",
    "                 ('tablet_density', 'v18q1', 'r4t3'),\n",
    "                 ('mobile_adult_density', 'qmobilephone', 'r4t2'),\n",
    "                 ('tablet_adult_density', 'v18q1', 'r4t2'),\n",
    "                 #('', '', ''),\n",
    "                ]\n",
    "    \n",
    "    feats_sub = [('people_not_living', 'tamhog', 'tamviv'),\n",
    "                 ('people_weird_stat', 'tamhog', 'r4t3')]\n",
    "\n",
    "    for f_new, f1, f2 in feats_div:\n",
    "        df['fe_' + f_new] = (df[f1] / df[f2]).astype(np.float32)       \n",
    "    for f_new, f1, f2 in feats_sub:\n",
    "        df['fe_' + f_new] = (df[f1] - df[f2]).astype(np.float32)\n",
    "    \n",
    "    # aggregation rules over household\n",
    "    aggs_num = {'age': ['min', 'max', 'mean', 'count'],\n",
    "                'escolari': ['min', 'max', 'mean']\n",
    "               }\n",
    "    aggs_cat = {'dis': ['mean', 'sum']}\n",
    "    for s_ in ['estadocivil', 'parentesco', 'instlevel']:\n",
    "        for f_ in [f_ for f_ in df.columns if f_.startswith(s_)]:\n",
    "            aggs_cat[f_] = ['mean']\n",
    "    # aggregation over household\n",
    "    for name_, df_ in [('18', df.query('age >= 18'))]:\n",
    "        df_agg = df_.groupby('idhogar').agg({**aggs_num, **aggs_cat}).astype(np.float32)\n",
    "        df_agg.columns = pd.Index(['agg' + name_ + '_' + e[0] + \"_\" + e[1].upper() for e in df_agg.columns.tolist()])\n",
    "        df = df.join(df_agg, how='left', on='idhogar')\n",
    "        del df_agg\n",
    "    # do something advanced above...\n",
    "    \n",
    "    # Drop SQB variables, as they are just squres of other vars \n",
    "    df.drop([f_ for f_ in df.columns if f_.startswith('SQB') or f_ == 'agesq'], axis=1, inplace=True)\n",
    "    # Drop id's\n",
    "    df.drop(['Id'], axis=1, inplace=True)\n",
    "    # Drop repeated columns\n",
    "    df.drop(['hhsize', 'female', 'area2'], axis=1, inplace=True)\n",
    "    return df\n",
    "    \n",
    "def convert_OHE2LE(df):\n",
    "    tmp_df = df.copy(deep=True)\n",
    "    for s_ in ['pared', 'piso', 'techo', 'abastagua', 'sanitario', 'energcocinar', 'elimbasu', \n",
    "               'epared', 'etecho', 'eviv', 'estadocivil', 'parentesco', \n",
    "               'instlevel', 'lugar', 'tipovivi',\n",
    "               'manual_elec']:\n",
    "        if 'manual_' not in s_:\n",
    "            cols_s_ = [f_ for f_ in df.columns if f_.startswith(s_)]\n",
    "        elif 'elec' in s_:\n",
    "            cols_s_ = ['public', 'planpri', 'noelec', 'coopele']\n",
    "        sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n",
    "        #deal with those OHE, where there is a sum over columns == 0\n",
    "        if 0 in sum_ohe:\n",
    "            print('The OHE in {} is incomplete. A new column will be added before label encoding'\n",
    "                  .format(s_))\n",
    "            # dummy colmn name to be added\n",
    "            col_dummy = s_+'_dummy'\n",
    "            # add the column to the dataframe\n",
    "            tmp_df[col_dummy] = (tmp_df[cols_s_].sum(axis=1) == 0).astype(np.int8)\n",
    "            # add the name to the list of columns to be label-encoded\n",
    "            cols_s_.append(col_dummy)\n",
    "            # proof-check, that now the category is complete\n",
    "            sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n",
    "            if 0 in sum_ohe:\n",
    "                 print(\"The category completion did not work\")\n",
    "        tmp_cat = tmp_df[cols_s_].idxmax(axis=1)\n",
    "        tmp_df[s_ + '_LE'] = LabelEncoder().fit_transform(tmp_cat).astype(np.int16)\n",
    "        if 'parentesco1' in cols_s_:\n",
    "            cols_s_.remove('parentesco1')\n",
    "        tmp_df.drop(cols_s_, axis=1, inplace=True)\n",
    "    return tmp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "eab84429fc9893c82e33b8319161c190b4104e9f"
   },
   "source": [
    "# Read in the data and clean it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e6f696a1677230c565532f141a02852e7c69b2e1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bd1f66cbdbfa4741d19a8b1f53793b967d62d281",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_df(df_):\n",
    "    # fix categorical features\n",
    "    encode_data(df_)\n",
    "    #fill in missing values based on https://www.kaggle.com/mlisovyi/missing-values-in-the-data\n",
    "    for f_ in ['v2a1', 'v18q1', 'meaneduc', 'SQBmeaned']:\n",
    "        df_[f_] = df_[f_].fillna(0)\n",
    "    df_['rez_esc'] = df_['rez_esc'].fillna(-1)\n",
    "    # do feature engineering and drop useless columns\n",
    "    return do_features(df_)\n",
    "\n",
    "train = process_df(train)\n",
    "test = process_df(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "640e1e5866802c21d094a6adb2035af93daf1360",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_apply_func(train_, test_, func_):\n",
    "    test_['Target'] = 0\n",
    "    xx = pd.concat([train_, test_])\n",
    "\n",
    "    xx_func = func_(xx)\n",
    "    train_ = xx_func.iloc[:train_.shape[0], :]\n",
    "    test_  = xx_func.iloc[train_.shape[0]:, :].drop('Target', axis=1)\n",
    "\n",
    "    del xx, xx_func\n",
    "    return train_, test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "23a970f220e3821930132f3f1e3d97886748e379",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_apply_func(train, test, convert_OHE2LE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "65dab0e9a94e8f87a7b73e7ec2c6559e4ccef996",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.info(max_cols=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "19f52b05ce1aca4210ba373db60a8be75d0e4a46"
   },
   "source": [
    "# Geo aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5fdbbf9ec2b45814882c45ed9b83eb51a95b1691",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_2_ohe = ['eviv_LE', 'etecho_LE', 'epared_LE', 'elimbasu_LE', \n",
    "              'energcocinar_LE', 'sanitario_LE', 'manual_elec_LE',\n",
    "              'pared_LE']\n",
    "cols_nums = ['age', 'meaneduc', 'dependency', \n",
    "             'hogar_nin', 'hogar_adul', 'hogar_mayor', 'hogar_total',\n",
    "             'bedrooms', 'overcrowding']\n",
    "\n",
    "def convert_geo2aggs(df_):\n",
    "    tmp_df = pd.concat([df_[(['lugar_LE', 'idhogar']+cols_nums)],\n",
    "                        pd.get_dummies(df_[cols_2_ohe], \n",
    "                                       columns=cols_2_ohe)],axis=1)\n",
    "    #print(pd.get_dummies(train[cols_2_ohe], \n",
    "    #                                   columns=cols_2_ohe).head())\n",
    "    #print(tmp_df.head())\n",
    "    #print(tmp_df.groupby(['lugar_LE','idhogar']).mean().head())\n",
    "    geo_agg = tmp_df.groupby(['lugar_LE','idhogar']).mean().groupby('lugar_LE').mean().astype(np.float32)\n",
    "    geo_agg.columns = pd.Index(['geo_' + e + '_MEAN' for e in geo_agg.columns.tolist()])\n",
    "    \n",
    "    #print(gb.T)\n",
    "    del tmp_df\n",
    "    return df_.join(geo_agg, how='left', on='lugar_LE')\n",
    "\n",
    "train, test = train_test_apply_func(train, test, convert_geo2aggs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9858e0b145850825a201df702ffd1eddc4ff6eba"
   },
   "source": [
    "# VERY IMPORTANT\n",
    "> Note that **ONLY the heads of household are used in scoring**. All household members are included in test + the sample submission, but only heads of households are scored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "96e8311b0d5cdddcf98b03d47d5e4793f0b79f03",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = train.query('parentesco1==1')\n",
    "\n",
    "# pull out the target variable\n",
    "y = X['Target'] - 1 # this is done to bing input labels [1,2,3,4] in agreement with lightgbm [0,1,2,3]\n",
    "X = X.drop(['Target'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "21e95be3582e0bd1bb512b4a70db1aab4c042652",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cols_2_drop = ['agg18_estadocivil1_MEAN', 'agg18_estadocivil3_COUNT', 'agg18_estadocivil4_COUNT', 'agg18_estadocivil5_COUNT', 'agg18_estadocivil6_COUNT', 'agg18_estadocivil7_COUNT', 'agg18_instlevel1_COUNT', 'agg18_instlevel2_COUNT', 'agg18_instlevel3_COUNT', 'agg18_instlevel4_COUNT', 'agg18_instlevel5_COUNT', 'agg18_instlevel6_COUNT', 'agg18_instlevel7_COUNT', 'agg18_instlevel8_COUNT', 'agg18_instlevel9_COUNT', 'agg18_parentesco10_COUNT', 'agg18_parentesco10_MEAN', 'agg18_parentesco11_COUNT', 'agg18_parentesco11_MEAN', 'agg18_parentesco12_COUNT', 'agg18_parentesco12_MEAN', 'agg18_parentesco1_COUNT', 'agg18_parentesco2_COUNT', 'agg18_parentesco3_COUNT', 'agg18_parentesco4_COUNT', 'agg18_parentesco4_MEAN', 'agg18_parentesco5_COUNT', 'agg18_parentesco6_COUNT', 'agg18_parentesco6_MEAN', 'agg18_parentesco7_COUNT', 'agg18_parentesco7_MEAN', 'agg18_parentesco8_COUNT', 'agg18_parentesco8_MEAN', 'agg18_parentesco9_COUNT', 'fe_people_weird_stat', 'hacapo', 'hacdor', 'mobilephone', 'parentesco1', 'parentesco_LE', 'rez_esc', 'v14a', 'v18q']\n",
    "cols_2_drop = ['abastagua_LE', 'agg18_estadocivil1_MEAN', 'agg18_instlevel6_MEAN', 'agg18_parentesco10_MEAN', 'agg18_parentesco11_MEAN', 'agg18_parentesco12_MEAN', 'agg18_parentesco4_MEAN', 'agg18_parentesco5_MEAN', 'agg18_parentesco6_MEAN', 'agg18_parentesco7_MEAN', 'agg18_parentesco8_MEAN', 'agg18_parentesco9_MEAN', 'fe_people_not_living', 'fe_people_weird_stat', 'geo_elimbasu_LE_3_MEAN', 'geo_elimbasu_LE_4_MEAN', 'geo_energcocinar_LE_0_MEAN', 'geo_energcocinar_LE_1_MEAN', 'geo_energcocinar_LE_2_MEAN', 'geo_epared_LE_0_MEAN', 'geo_epared_LE_2_MEAN', 'geo_etecho_LE_2_MEAN', 'geo_eviv_LE_0_MEAN', 'geo_hogar_mayor_MEAN', 'geo_hogar_nin_MEAN', 'geo_manual_elec_LE_1_MEAN', 'geo_manual_elec_LE_2_MEAN', 'geo_manual_elec_LE_3_MEAN', 'geo_pared_LE_0_MEAN', 'geo_pared_LE_1_MEAN', 'geo_pared_LE_3_MEAN', 'geo_pared_LE_4_MEAN', 'geo_pared_LE_5_MEAN', 'geo_pared_LE_6_MEAN', 'geo_pared_LE_7_MEAN', 'hacapo', 'hacdor', 'mobilephone', 'parentesco1', 'parentesco_LE', 'rez_esc', 'techo_LE', 'v14a', 'v18q']\n",
    "\n",
    "X.drop((cols_2_drop+['idhogar']), axis=1, inplace=True)\n",
    "test.drop((cols_2_drop+['idhogar']), axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c6e1ccce811e7a1d76282fcb8a13edf92672f834"
   },
   "source": [
    "# Model fitting with HyperParameter optimisation\n",
    "\n",
    "We will use LightGBM classifier - LightGBM allows to build very sophysticated models with a very short training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ae8c1b6d6f95654ab6f0ba36238886969a556a2e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=314, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8b18d9eb9e4b7429d53b7151976ee93d171ff6c3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test.info(max_cols=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5afc6fbb6ed16a47c0700e7ffc1d26cb2e28e778"
   },
   "source": [
    "## Use test subset for early stopping criterion\n",
    "\n",
    "This allows us to avoid overtraining and we do not need to optimise the number of trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "76956b6fa33cc0dcedc3602f34c4be96f6558778",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def evaluate_macroF1_lgb(truth, predictions):  \n",
    "    # this follows the discussion in https://github.com/Microsoft/LightGBM/issues/1483\n",
    "    pred_labels = predictions.reshape(len(np.unique(truth)),-1).argmax(axis=0)\n",
    "    f1 = f1_score(truth, pred_labels, average='macro')\n",
    "    return ('macroF1', f1, True) \n",
    "\n",
    "import lightgbm as lgb\n",
    "fit_params={\"early_stopping_rounds\":300, \n",
    "            \"eval_metric\" : evaluate_macroF1_lgb, \n",
    "            \"eval_set\" : [(X_test,y_test)],\n",
    "            'eval_names': ['valid'],\n",
    "            #'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_099)],\n",
    "            'verbose': False,\n",
    "            'categorical_feature': 'auto'}\n",
    "\n",
    "def learning_rate_power_0997(current_iter):\n",
    "    base_learning_rate = 0.1\n",
    "    min_learning_rate = 0.02\n",
    "    lr = base_learning_rate  * np.power(.995, current_iter)\n",
    "    return max(lr, min_learning_rate)\n",
    "\n",
    "fit_params['callbacks'] = [lgb.reset_parameter(learning_rate=learning_rate_power_0997)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "effe8ad863afc3f72e16ca3423588cfddd13408f"
   },
   "source": [
    "# Set up HyperParameter search\n",
    "\n",
    "We use random search, which is more flexible and more efficient than a grid search\n",
    "Define the distribution of parameters to be sampled from\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "22b494fee8b8880e56ce049631d7f161caef0554",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "param_test ={'num_leaves': sp_randint(12, 20), \n",
    "             'min_child_samples': sp_randint(40, 100), \n",
    "             #'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "             'subsample': sp_uniform(loc=0.75, scale=0.25), \n",
    "             'colsample_bytree': sp_uniform(loc=0.8, scale=0.15)#,\n",
    "             #'reg_alpha': [0, 1e-3, 1e-1, 1, 10, 50, 100],\n",
    "             #'reg_lambda': [0, 1e-3, 1e-1, 1, 10, 50, 100]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1f1da76da60d26da9c28b5d396131a7aa70553e9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This parameter defines the number of HP points to be tested\n",
    "n_HP_points_to_test = 100\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "clf = lgb.LGBMClassifier(max_depth=-1, learning_rate=0.05, objective='multiclass',\n",
    "                         random_state=314, silent=True, metric='None', \n",
    "                         n_jobs=4, n_estimators=5000, class_weight='balanced')\n",
    "\n",
    "gs = RandomizedSearchCV(\n",
    "    estimator=clf, param_distributions=param_test, \n",
    "    n_iter=n_HP_points_to_test,\n",
    "    scoring='f1_macro',\n",
    "    cv=5,\n",
    "    refit=True,\n",
    "    random_state=314,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2da0c546c564afaabc75624a91ecc226a0db8e6b"
   },
   "source": [
    "The actual search for the optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e0b36f0a58ac3041567cddd39a615e6f77425c57",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = gs.fit(X_train, y_train, **fit_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "14b7de781555bf6df33467f08ca3e952725dbef0"
   },
   "source": [
    "Let's print the 'top 5 parameter configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "68129b16c4b855745c96579e09138ae88c4584ab",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"PERFORMANCE IMPROVES FROM TOP TO BOTTOM\")\n",
    "print(\"Valid+-Std     Train  :   Parameters\")\n",
    "for i in np.argsort(gs.cv_results_['mean_test_score'])[-5:]:\n",
    "    print('{1:.3f}+-{3:.3f}     {2:.3f}   :  {0}'.format(gs.cv_results_['params'][i], \n",
    "                                    gs.cv_results_['mean_test_score'][i], \n",
    "                                    gs.cv_results_['mean_train_score'][i],\n",
    "                                    gs.cv_results_['std_test_score'][i]))\n",
    "\n",
    "opt_parameters = gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b1261bdd85328e83f841f257b102e63864b8e31d",
    "collapsed": true
   },
   "source": [
    "# Fit the final model with learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "37b909c2aa273651b7bb57c69b939760f14f38f7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_final = lgb.LGBMClassifier(**clf.get_params())\n",
    "clf_final.set_params(**opt_parameters)\n",
    "\n",
    "def learning_rate_power_0997(current_iter):\n",
    "    base_learning_rate = 0.1\n",
    "    min_learning_rate = 0.02\n",
    "    lr = base_learning_rate  * np.power(.997, current_iter)\n",
    "    return max(lr, min_learning_rate)\n",
    "\n",
    "#Train the final model with learning rate decay\n",
    "fit_params['verbose'] = 200\n",
    "_ = clf_final.fit(X_train, y_train, **fit_params)#, callbacks=[lgb.reset_parameter(learning_rate=learning_rate_power_0997)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "78749eec7f69bcc8c587278a2c1a43ac8b5832e3"
   },
   "source": [
    "# Prepare submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "32bfd69fe130005cb88865399c460ac00c7b1574",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_subm = pd.read_csv('../input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "718054937aa849c23ca7c65483f8a52de6f6fd49",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_subm['Target'] = clf_final.predict(test) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8258f28127f235e427a25f6824566a23df61b8af",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "global_score = f1_score(y_test, clf_final.predict(X_test), average='macro')\n",
    "\n",
    "sub_file = 'submission_LGB_{:.4f}_{}.csv'.format(global_score, str(now.strftime('%Y-%m-%d-%H-%M')))\n",
    "\n",
    "y_subm.to_csv(sub_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3c6d7f945dec95a777b4221c5fe217c3eea24100",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
