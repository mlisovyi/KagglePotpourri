{"cells":[{"metadata":{"_uuid":"0479e25c001874fda15f875d76f8663885caa138"},"cell_type":"markdown","source":"# Do feature engineering to improve LightGBM prediction\nThis kernel closely follows https://www.kaggle.com/mlisovyi/feature-engineering-lighgbm-with-f1-macro, but instead of making accent on precise prediction, we look what is going on under the hood. \n\nThe key points:\n- The main point is **to demonstrate the origin of low F1 scores that we achieve on these data**\n- The studies start here: [Confusion matrix studies](#F1-score-across-different-classes)\n- Spoiler: no magical solution. Any feedback/advice is welcome"},{"metadata":{"_uuid":"1de703977546e06fce7486fabe0b500e015191f6"},"cell_type":"markdown","source":"# Initial imports and helper functions"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd \n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e4e08a17549fd247619178c96c3ade2519e9773"},"cell_type":"markdown","source":"The following categorical mapping originates from [this kernel](https://www.kaggle.com/mlisovyi/categorical-variables-encoding-function)."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\ndef encode_data(df):\n    '''\n    The function does not return, but transforms the input pd.DataFrame\n    \n    Encodes the Costa Rican Household Poverty Level data \n    following studies in https://www.kaggle.com/mlisovyi/categorical-variables-in-the-data\n    and the insight from https://www.kaggle.com/c/costa-rican-household-poverty-prediction/discussion/61403#359631\n    \n    The following columns get transformed: edjefe, edjefa, dependency, idhogar\n    The user most likely will simply drop idhogar completely (after calculating houshold-level aggregates)\n    '''\n    \n    yes_no_map = {'no': 0, 'yes': 1}\n    \n    df['dependency'] = df['dependency'].replace(yes_no_map).astype(np.float32)\n    \n    df['edjefe'] = df['edjefe'].replace(yes_no_map).astype(np.float32)\n    df['edjefa'] = df['edjefa'].replace(yes_no_map).astype(np.float32)\n    \n    df['idhogar'] = LabelEncoder().fit_transform(df['idhogar'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1785c8ca3467a7e95d007a2c5f36e39919fc0910"},"cell_type":"markdown","source":"**Drop less important features:**"},{"metadata":{"_kg_hide-input":true,"trusted":true,"collapsed":true,"_uuid":"9c9f13e54fc2af9f938b895959631e1aeb3b08a2"},"cell_type":"code","source":"def drop_features(df):\n    # Drop SQB variables, as they are just squres of other vars \n    df.drop([f_ for f_ in df.columns if f_.startswith('SQB') or f_ == 'agesq'], axis=1, inplace=True)\n    # Drop id's\n    df.drop(['Id', 'idhogar'], axis=1, inplace=True)\n    # Drop repeated columns\n    df.drop(['hhsize', 'female', 'area2'], axis=1, inplace=True)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"collapsed":true,"_uuid":"a7df32a07c9157ee02ff9688cdc70c69e7571aae"},"cell_type":"code","source":"def convert_OHE2LE(df):\n    tmp_df = df.copy(deep=True)\n    for s_ in ['pared', 'piso', 'techo', 'abastagua', 'sanitario', 'energcocinar', 'elimbasu', \n               'epared', 'etecho', 'eviv', 'estadocivil', 'parentesco', \n               'instlevel', 'lugar', 'tipovivi',\n               'manual_elec']:\n        if 'manual_' not in s_:\n            cols_s_ = [f_ for f_ in df.columns if f_.startswith(s_)]\n        elif 'elec' in s_:\n            cols_s_ = ['public', 'planpri', 'noelec', 'coopele']\n        sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n        #deal with those OHE, where there is a sum over columns == 0\n        if 0 in sum_ohe:\n            print('The OHE in {} is incomplete. A new column will be added before label encoding'\n                  .format(s_))\n            # dummy colmn name to be added\n            col_dummy = s_+'_dummy'\n            # add the column to the dataframe\n            tmp_df[col_dummy] = (tmp_df[cols_s_].sum(axis=1) == 0).astype(np.int8)\n            # add the name to the list of columns to be label-encoded\n            cols_s_.append(col_dummy)\n            # proof-check, that now the category is complete\n            sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n            if 0 in sum_ohe:\n                 print(\"The category completion did not work\")\n        tmp_cat = tmp_df[cols_s_].idxmax(axis=1)\n        tmp_df[s_ + '_LE'] = LabelEncoder().fit_transform(tmp_cat).astype(np.int16)\n        if 'parentesco1' in cols_s_:\n            cols_s_.remove('parentesco1')\n        tmp_df.drop(cols_s_, axis=1, inplace=True)\n    return tmp_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eab84429fc9893c82e33b8319161c190b4104e9f"},"cell_type":"markdown","source":"# Read in the data and clean it up"},{"metadata":{"trusted":true,"_uuid":"e6f696a1677230c565532f141a02852e7c69b2e1","collapsed":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"334238c8c677787a13a08a621827c6f1b0565046","collapsed":true},"cell_type":"code","source":"train.info(verbose=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd1f66cbdbfa4741d19a8b1f53793b967d62d281","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"def process_df(df_):\n    # fix categorical features\n    encode_data(df_)\n    #fill in missing values based on https://www.kaggle.com/mlisovyi/missing-values-in-the-data\n    for f_ in ['v2a1', 'v18q1', 'meaneduc', 'SQBmeaned']:\n        df_[f_] = df_[f_].fillna(0)\n    df_['rez_esc'] = df_['rez_esc'].fillna(-1)\n    # drop useless columns\n    return drop_features(df_)\n\ntrain = process_df(train)\ntest = process_df(test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"809763b29c54f5a15da2c5670407eaf95c62cfc2"},"cell_type":"markdown","source":"Now, let's define `train_test_apply_func` helper function to apply a custom function to a concatenated test+train dataset"},{"metadata":{"trusted":true,"_uuid":"b564a6552f0521581af1ee38d6040ef7ae5d2fb5","collapsed":true,"_kg_hide-input":true},"cell_type":"code","source":"def train_test_apply_func(train_, test_, func_):\n    test_['Target'] = 0\n    xx = pd.concat([train_, test_])\n\n    xx_func = func_(xx)\n    train_ = xx_func.iloc[:train_.shape[0], :]\n    test_  = xx_func.iloc[train_.shape[0]:, :].drop('Target', axis=1)\n\n    del xx, xx_func\n    return train_, test_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2695108e103c9c61088fc4c100d01bc8c0f4138c","collapsed":true},"cell_type":"code","source":"train, test = train_test_apply_func(train, test, convert_OHE2LE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2acd39c04f144669e58a0b9e1129a20e664137c9","collapsed":true},"cell_type":"code","source":"train.info(verbose=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1aa466a92793efd57dc3e58733a58d1fb869c278"},"cell_type":"markdown","source":"Compare the number of features with `int64` type to the previous info summary. The difference comes from convertion of OHE into LE (`convert_OHE2LE` function)"},{"metadata":{"_uuid":"9858e0b145850825a201df702ffd1eddc4ff6eba"},"cell_type":"markdown","source":"# VERY IMPORTANT\n> Note that ONLY the heads of household are used in scoring. All household members are included in test + the sample submission, but only heads of households are scored."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"96e8311b0d5cdddcf98b03d47d5e4793f0b79f03"},"cell_type":"code","source":"X = train.query('parentesco1==1')\n\n# pull out the target variable\ny = X['Target'] - 1\nX = X.drop(['Target'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6e1ccce811e7a1d76282fcb8a13edf92672f834"},"cell_type":"markdown","source":"# Model fitting\n\nWe will use LightGBM classifier - LightGBM allows to build very sophysticated models with a very short training time."},{"metadata":{"_uuid":"5afc6fbb6ed16a47c0700e7ffc1d26cb2e28e778"},"cell_type":"markdown","source":"## Use test subset for early stopping criterion\n\nThis allows us to avoid overtraining and we do not need to optimise the number of trees. We also use F1 macro-averaged score to decide when to stop\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4f44b915743fccd13ec3a6ef607acbdf8af87bb9"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=314, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"76956b6fa33cc0dcedc3602f34c4be96f6558778","_kg_hide-input":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\ndef evaluate_macroF1_lgb(truth, predictions):  \n    # this follows the discussion in https://github.com/Microsoft/LightGBM/issues/1483\n    pred_labels = predictions.reshape(len(np.unique(truth)),-1).argmax(axis=0)\n    f1 = f1_score(truth, pred_labels, average='macro')\n    return ('macroF1', f1, True) \n\nimport lightgbm as lgb\nfit_params={\"early_stopping_rounds\":300, \n            \"eval_metric\" : evaluate_macroF1_lgb, \n            \"eval_set\" : [(X_train,y_train), (X_test,y_test)],\n            'eval_names': ['train', 'early_stop'],\n            'verbose': 100,\n            'categorical_feature': 'auto'}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"effe8ad863afc3f72e16ca3423588cfddd13408f"},"cell_type":"markdown","source":"# LightGBM optimal parameters\n\nThe parameters are optimised with a random search in this kernel: https://www.kaggle.com/mlisovyi/lighgbm-hyperoptimisation-with-f1-macro\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"22b494fee8b8880e56ce049631d7f161caef0554"},"cell_type":"code","source":"opt_parameters = {'colsample_bytree': 0.89, 'min_child_samples': 90, 'num_leaves': 14, 'subsample': 0.96}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2dc384aeb44db2454978df78fdbb84b2b1ff3ced"},"cell_type":"markdown","source":"# Fit a LGBM classifier"},{"metadata":{"trusted":true,"_uuid":"61bd19ba9f1219a29b7ef412120ec92c070fc35c","_kg_hide-output":false,"_kg_hide-input":false,"collapsed":true},"cell_type":"code","source":"def train_lgbm_model(X_, y_, random_state_=None, opt_parameters_={}, fit_params_={}, lr_=0.05):\n    clf  = lgb.LGBMClassifier(max_depth=-1, learning_rate=lr_, objective='multiclass',\n                             random_state=random_state_, silent=True, metric='None', \n                             n_jobs=4, n_estimators=5000, class_weight='balanced')\n    clf.set_params(**opt_parameters_)\n    return clf.fit(X_, y_, **fit_params_)\n\nclf_final = train_lgbm_model(X_train, y_train, \n                       random_state_=314, \n                       opt_parameters_=opt_parameters,\n                       fit_params_=fit_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"681d9bbe0fa31d443feefb52ee698975bce21e2f","collapsed":true},"cell_type":"markdown","source":"So the final macro F_1 score on the early-stop test sample is `~0.44`\n# F1 score across different classes\nLet's see if all classes show similar performance"},{"metadata":{"trusted":true,"_uuid":"a71386355812dd97c74737afc7bf01b94239346e","collapsed":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\ndef print_report(clf_, X_tr, y_tr, X_tt, y_tt):\n    print('------------ Train sample -------------\\n', \n          classification_report(y_tr, clf_.predict(X_tr)))\n    print('------------ Test sample -------------\\n', \n          classification_report(y_tt, clf_.predict(X_tt)))\n\nprint_report(clf_final, X_train, y_train, X_test, y_test) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4effcaeed03f89f788116334f7886b7d33788fc"},"cell_type":"markdown","source":"As we can see:\n\n - in the train sample the the model is able to predict all classes with roughly the same F1 score (the third column)\n - in the test sample only the last class (*\"non vulnerable households\"*) can be predicted reliably, while others have very low precision and recall\n \n Supposedly this is due to poor separation between three poorest classes, as is shown in this kernel: https://www.kaggle.com/mlisovyi/cluster-analysis-tsne-mds-isomap\n \n How can we address it?\n \n \n# Let's try to build a model to distinguish between poor classes only:"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"35b8b7ee58a72c332d40203a2809b94d7f057805"},"cell_type":"code","source":"def drop_classes(X_, y_, to_drop_=3):\n    XY = pd.concat([X_, y_], axis=1)\n    XY = XY.query('Target != @to_drop_')\n    return XY.drop('Target', axis=1), XY['Target']\n\nX_train_wo3, y_train_wo3 = drop_classes(X_train, y_train, 3)\nX_test_wo3,  y_test_wo3  = drop_classes(X_test, y_test, 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91f52d242cb84de930cc99f18810436f20697ac4","collapsed":true},"cell_type":"code","source":"import copy\nfit_params_wo3 = copy.deepcopy(fit_params)\nfit_params_wo3['eval_set'] = [(X_train_wo3, y_train_wo3), (X_test_wo3, y_test_wo3)]\nfit_params_wo3['verbose'] = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"581b7015bd0bd5cfa37b9e503a19b0b465750567","collapsed":true},"cell_type":"code","source":"clf_wo3 = train_lgbm_model(X_train_wo3, y_train_wo3, \n                       random_state_=314, \n                       opt_parameters_=opt_parameters,\n                       fit_params_=fit_params_wo3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90e0754b349fc5d6b511aa9930a40de799c6f07f","collapsed":true},"cell_type":"code","source":"print_report(clf_final, X_train_wo3, y_train_wo3, X_test_wo3, y_test_wo3) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4718f62987b15bfa8a8335d9fa2e146a47add140"},"cell_type":"markdown","source":"# Nope, the model can not well distinguish between the 3 poorest classes"},{"metadata":{"trusted":true,"_uuid":"430892299b4a617985a19cbca37ecd2f8d249859"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e8f714d832b01610f60d958dd0f82cbae84db98"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}