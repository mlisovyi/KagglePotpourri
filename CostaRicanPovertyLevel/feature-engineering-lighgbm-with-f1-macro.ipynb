{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0479e25c001874fda15f875d76f8663885caa138"
   },
   "source": [
    "# Do feature engineering to improve LightGBM prediction\n",
    "This kernel closely follows https://www.kaggle.com/mlisovyi/lighgbm-hyperoptimisation-with-f1-macro, but instead of running hyperparameter optimisation it uses optimal values from that kernel and thus runs faster. \n",
    "\n",
    "Several key points:\n",
    "- **This kernel runs training on the heads of housholds only** (after extracting aggregates over households). This follows the announced scoring startegy: *Note that ONLY the heads of household are used in scoring. All household members are included in test + the sample submission, but only heads of households are scored.* (from the data description). \n",
    "- **It seems to be very important to balance class frequencies.** Without balancing a trained model gives ~0.39 PLB / ~0.43 local test, while adding balancing leads to ~0.42 PLB / 0.47 local test. One can do it by hand, one can achieve it by undersampling. But the simplest (and more powerful compared to undersampling) is to set `class_weight='balanced'` in the LightGBM model constructor in sklearn API, which will assign different weights to different classes proportional to their representation. *Note that a better procedure would be to tune those weights in a CV loop instead of blindly assigning 1/n weights*\n",
    "- **This kernel uses macro F1 score to early stopping in training**. This is done to align with the scoring strategy.\n",
    "- Categoricals are turned into numbers with proper mapping instead of blind label encoding. \n",
    "- **OHE is reversed into label encoding, as it is easier to digest for a tree model.** This trick would be harmful for non-tree models, so be careful.\n",
    "- **idhogar is NOT used in training**. The only way it could have any info would be if there is a data leak. We are fighting with poverty here- exploiting leaks will not reduce poverty in any way :)\n",
    "- **Squared features (`SQBXXX` and `agesq`) are NOT used in training**. These would be useful for a linear model, but are useless for a tree-based model and only confused it (when bagging and resampling is done)\n",
    "- **There are aggregations done within households and new features are hand-crafted**. Note, that there are not so many features that can be aggregated, as most are already quoted on household level.\n",
    "- **NEW: There are geographical aggregates calculated from households**\n",
    "- **NEW: Models are build and evaluated in a nested CV loop**. This is done to reduce fluctuations in early-stopping criterion as well as to average over several performance estimates.\n",
    "- **A voting classifier is used to average over several LightGBM models**\n",
    "\n",
    "The main goal is to do feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8e4e08a17549fd247619178c96c3ade2519e9773"
   },
   "source": [
    "The following categorical mapping originates from [this kernel](https://www.kaggle.com/mlisovyi/categorical-variables-encoding-function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def encode_data(df):\n",
    "    '''\n",
    "    The function does not return, but transforms the input pd.DataFrame\n",
    "    \n",
    "    Encodes the Costa Rican Household Poverty Level data \n",
    "    following studies in https://www.kaggle.com/mlisovyi/categorical-variables-in-the-data\n",
    "    and the insight from https://www.kaggle.com/c/costa-rican-household-poverty-prediction/discussion/61403#359631\n",
    "    \n",
    "    The following columns get transformed: edjefe, edjefa, dependency, idhogar\n",
    "    The user most likely will simply drop idhogar completely (after calculating houshold-level aggregates)\n",
    "    '''\n",
    "    \n",
    "    yes_no_map = {'no': 0, 'yes': 1}\n",
    "    \n",
    "    df['dependency'] = df['dependency'].replace(yes_no_map).astype(np.float32)\n",
    "    \n",
    "    df['edjefe'] = df['edjefe'].replace(yes_no_map).astype(np.float32)\n",
    "    df['edjefa'] = df['edjefa'].replace(yes_no_map).astype(np.float32)\n",
    "    \n",
    "    df['idhogar'] = LabelEncoder().fit_transform(df['idhogar'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1785c8ca3467a7e95d007a2c5f36e39919fc0910"
   },
   "source": [
    "**There is also feature engineering magic happening here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "9c9f13e54fc2af9f938b895959631e1aeb3b08a2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_features(df):\n",
    "    feats_div = [('children_fraction', 'r4t1', 'r4t3'), \n",
    "                 ('working_man_fraction', 'r4h2', 'r4t3'),\n",
    "                 ('all_man_fraction', 'r4h3', 'r4t3'),\n",
    "                 ('human_density', 'tamviv', 'rooms'),\n",
    "                 ('human_bed_density', 'tamviv', 'bedrooms'),\n",
    "                 ('bed_density', 'bedrooms', 'rooms'),\n",
    "                 ('rent_per_person', 'v2a1', 'r4t3'),\n",
    "                 ('rent_per_room', 'v2a1', 'rooms'),\n",
    "                 ('mobile_density', 'qmobilephone', 'r4t3'),\n",
    "                 ('tablet_density', 'v18q1', 'r4t3'),\n",
    "                 ('mobile_adult_density', 'qmobilephone', 'r4t2'),\n",
    "                 ('tablet_adult_density', 'v18q1', 'r4t2'),\n",
    "                 ('male_over_female', 'r4h3', 'r4m3'),\n",
    "                 ('man12plus_over_women12plus', 'r4h2', 'r4m2'),\n",
    "                 ('pesioner_over_working', 'hogar_mayor', 'hogar_adul'),\n",
    "                 ('children_over_working', 'hogar_nin', 'hogar_adul'),\n",
    "                 ('education_fraction', 'escolari', 'age')\n",
    "                 #('', '', ''),\n",
    "                ]\n",
    "    \n",
    "    feats_sub = [('people_not_living', 'tamhog', 'tamviv'),\n",
    "                 ('non_bedrooms', 'rooms', 'bedrooms'),\n",
    "                 ('people_weird_stat', 'tamhog', 'r4t3')]\n",
    "\n",
    "    for f_new, f1, f2 in feats_div:\n",
    "        df['fe_' + f_new] = (df[f1] / df[f2]).astype(np.float32)       \n",
    "    for f_new, f1, f2 in feats_sub:\n",
    "        df['fe_' + f_new] = (df[f1] - df[f2]).astype(np.float32)\n",
    "    \n",
    "    # aggregation rules over household\n",
    "    aggs_num = {'age': ['min', 'max', 'mean', 'count'],\n",
    "                'escolari': ['min', 'max', 'mean', 'std'],\n",
    "                'fe_education_fraction': ['min', 'max', 'mean', 'std']\n",
    "               }\n",
    "    aggs_cat = {'dis': ['mean']}\n",
    "    for s_ in ['estadocivil', 'parentesco', 'instlevel']:\n",
    "        for f_ in [f_ for f_ in df.columns if f_.startswith(s_)]:\n",
    "            aggs_cat[f_] = ['mean']\n",
    "    # aggregation over household\n",
    "    for name_, df_ in [('18', df.query('age >= 18'))]:\n",
    "        df_agg = df_.groupby('idhogar').agg({**aggs_num, **aggs_cat}).astype(np.float32)\n",
    "        df_agg.columns = pd.Index(['agg' + name_ + '_' + e[0] + \"_\" + e[1].upper() for e in df_agg.columns.tolist()])\n",
    "        df = df.join(df_agg, how='left', on='idhogar')\n",
    "        del df_agg\n",
    "    # do something advanced above...\n",
    "    \n",
    "    # Drop SQB variables, as they are just squres of other vars \n",
    "    df.drop([f_ for f_ in df.columns if f_.startswith('SQB') or f_ == 'agesq'], axis=1, inplace=True)\n",
    "    # Drop id's\n",
    "    df.drop(['Id'], axis=1, inplace=True)\n",
    "    # Drop repeated columns\n",
    "    df.drop(['hhsize', 'female', 'area2'], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "a7df32a07c9157ee02ff9688cdc70c69e7571aae",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_OHE2LE(df):\n",
    "    tmp_df = df.copy(deep=True)\n",
    "    for s_ in ['pared', 'piso', 'techo', 'abastagua', 'sanitario', 'energcocinar', 'elimbasu', \n",
    "               'epared', 'etecho', 'eviv', 'estadocivil', 'parentesco', \n",
    "               'instlevel', 'lugar', 'tipovivi',\n",
    "               'manual_elec']:\n",
    "        if 'manual_' not in s_:\n",
    "            cols_s_ = [f_ for f_ in df.columns if f_.startswith(s_)]\n",
    "        elif 'elec' in s_:\n",
    "            cols_s_ = ['public', 'planpri', 'noelec', 'coopele']\n",
    "        sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n",
    "        #deal with those OHE, where there is a sum over columns == 0\n",
    "        if 0 in sum_ohe:\n",
    "            print('The OHE in {} is incomplete. A new column will be added before label encoding'\n",
    "                  .format(s_))\n",
    "            # dummy colmn name to be added\n",
    "            col_dummy = s_+'_dummy'\n",
    "            # add the column to the dataframe\n",
    "            tmp_df[col_dummy] = (tmp_df[cols_s_].sum(axis=1) == 0).astype(np.int8)\n",
    "            # add the name to the list of columns to be label-encoded\n",
    "            cols_s_.append(col_dummy)\n",
    "            # proof-check, that now the category is complete\n",
    "            sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n",
    "            if 0 in sum_ohe:\n",
    "                 print(\"The category completion did not work\")\n",
    "        tmp_cat = tmp_df[cols_s_].idxmax(axis=1)\n",
    "        tmp_df[s_ + '_LE'] = LabelEncoder().fit_transform(tmp_cat).astype(np.int16)\n",
    "        if 'parentesco1' in cols_s_:\n",
    "            cols_s_.remove('parentesco1')\n",
    "        tmp_df.drop(cols_s_, axis=1, inplace=True)\n",
    "    return tmp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "eab84429fc9893c82e33b8319161c190b4104e9f"
   },
   "source": [
    "# Read in the data and clean it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e6f696a1677230c565532f141a02852e7c69b2e1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "334238c8c677787a13a08a621827c6f1b0565046",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bd1f66cbdbfa4741d19a8b1f53793b967d62d281",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_df(df_):\n",
    "    # fix categorical features\n",
    "    encode_data(df_)\n",
    "    #fill in missing values based on https://www.kaggle.com/mlisovyi/missing-values-in-the-data\n",
    "    for f_ in ['v2a1', 'v18q1', 'meaneduc', 'SQBmeaned']:\n",
    "        df_[f_] = df_[f_].fillna(0)\n",
    "    df_['rez_esc'] = df_['rez_esc'].fillna(-1)\n",
    "    # do feature engineering and drop useless columns\n",
    "    return do_features(df_)\n",
    "\n",
    "train = process_df(train)\n",
    "test = process_df(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "65dab0e9a94e8f87a7b73e7ec2c6559e4ccef996",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "809763b29c54f5a15da2c5670407eaf95c62cfc2"
   },
   "source": [
    "Note the change in the number of features of different type. What we did was:\n",
    "- encoded categorical variables appropreately into numerical values;\n",
    "- dropped a few irrelevant columns;\n",
    "- added several columns with household aggregates and cand-crafted ratio and subtraction features\n",
    "\n",
    "Now, let's define `train_test_apply_func` helper function to apply a custom function to a concatenated test+train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "b564a6552f0521581af1ee38d6040ef7ae5d2fb5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_apply_func(train_, test_, func_):\n",
    "    test_['Target'] = 0\n",
    "    xx = pd.concat([train_, test_])\n",
    "\n",
    "    xx_func = func_(xx)\n",
    "    train_ = xx_func.iloc[:train_.shape[0], :]\n",
    "    test_  = xx_func.iloc[train_.shape[0]:, :].drop('Target', axis=1)\n",
    "\n",
    "    del xx, xx_func\n",
    "    return train_, test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2695108e103c9c61088fc4c100d01bc8c0f4138c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_apply_func(train, test, convert_OHE2LE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2acd39c04f144669e58a0b9e1129a20e664137c9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1aa466a92793efd57dc3e58733a58d1fb869c278"
   },
   "source": [
    "Compare the number of features with `int64` type to the previous info summary. The difference comes from convertion of OHE into LE (`convert_OHE2LE` function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e90e68abd266c9db808dbf336308cef7175886bd"
   },
   "source": [
    "# Geo aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0ffc288b3829ffdb1dabf8f2e7fe518f2f520480",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_2_ohe = ['eviv_LE', 'etecho_LE', 'epared_LE', 'elimbasu_LE', \n",
    "              'energcocinar_LE', 'sanitario_LE', 'manual_elec_LE',\n",
    "              'pared_LE']\n",
    "cols_nums = ['age', 'meaneduc', 'dependency', \n",
    "             'hogar_nin', 'hogar_adul', 'hogar_mayor', 'hogar_total',\n",
    "             'bedrooms', 'overcrowding']\n",
    "\n",
    "def convert_geo2aggs(df_):\n",
    "    tmp_df = pd.concat([df_[(['lugar_LE', 'idhogar']+cols_nums)],\n",
    "                        pd.get_dummies(df_[cols_2_ohe], \n",
    "                                       columns=cols_2_ohe)],axis=1)\n",
    "    geo_agg = tmp_df.groupby(['lugar_LE','idhogar']).mean().groupby('lugar_LE').mean().astype(np.float32)\n",
    "    geo_agg.columns = pd.Index(['geo_' + e + '_MEAN' for e in geo_agg.columns.tolist()])\n",
    "    \n",
    "    del tmp_df\n",
    "    return df_.join(geo_agg, how='left', on='lugar_LE')\n",
    "\n",
    "train, test = train_test_apply_func(train, test, convert_geo2aggs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bc96d527090d9b0723049c5d763c97be5145b8c7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9858e0b145850825a201df702ffd1eddc4ff6eba"
   },
   "source": [
    "# VERY IMPORTANT\n",
    "> Note that ONLY the heads of household are used in scoring. All household members are included in test + the sample submission, but only heads of households are scored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "96e8311b0d5cdddcf98b03d47d5e4793f0b79f03",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = train.query('parentesco1==1')\n",
    "#X = train\n",
    "\n",
    "# pull out the target variable\n",
    "y = X['Target'] - 1\n",
    "X = X.drop(['Target'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5eaa7e4a95326459a7f2aeb24949e45cb237a9a4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_2_drop = ['abastagua_LE', 'agg18_estadocivil1_MEAN', 'agg18_instlevel6_MEAN', 'agg18_parentesco10_MEAN', 'agg18_parentesco11_MEAN', 'agg18_parentesco12_MEAN', 'agg18_parentesco4_MEAN', 'agg18_parentesco5_MEAN', 'agg18_parentesco6_MEAN', 'agg18_parentesco7_MEAN', 'agg18_parentesco8_MEAN', 'agg18_parentesco9_MEAN', 'fe_people_not_living', 'fe_people_weird_stat', 'geo_elimbasu_LE_3_MEAN', 'geo_elimbasu_LE_4_MEAN', 'geo_energcocinar_LE_0_MEAN', 'geo_energcocinar_LE_1_MEAN', 'geo_energcocinar_LE_2_MEAN', 'geo_epared_LE_0_MEAN', 'geo_epared_LE_2_MEAN', 'geo_etecho_LE_2_MEAN', 'geo_eviv_LE_0_MEAN', 'geo_hogar_mayor_MEAN', 'geo_hogar_nin_MEAN', 'geo_manual_elec_LE_1_MEAN', 'geo_manual_elec_LE_2_MEAN', 'geo_manual_elec_LE_3_MEAN', 'geo_pared_LE_0_MEAN', 'geo_pared_LE_1_MEAN', 'geo_pared_LE_3_MEAN', 'geo_pared_LE_4_MEAN', 'geo_pared_LE_5_MEAN', 'geo_pared_LE_6_MEAN', 'geo_pared_LE_7_MEAN', 'hacapo', 'hacdor', 'mobilephone', 'parentesco1', 'parentesco_LE', 'rez_esc', 'techo_LE', 'v14a', 'v18q']\n",
    "#cols_2_drop = ['agg18_estadocivil1_MEAN', 'agg18_parentesco10_MEAN', 'agg18_parentesco11_MEAN', 'agg18_parentesco12_MEAN', 'agg18_parentesco4_MEAN', 'agg18_parentesco6_MEAN', 'agg18_parentesco7_MEAN', 'agg18_parentesco8_MEAN', 'fe_people_weird_stat', 'hacapo', 'hacdor', 'mobilephone', 'parentesco1', 'parentesco_LE', 'rez_esc', 'v14a']\n",
    "#cols_2_drop=[]\n",
    "\n",
    "X.drop((cols_2_drop+['idhogar']), axis=1, inplace=True)\n",
    "test.drop((cols_2_drop+['idhogar']), axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b6fce8205f4e5ae78a43eef4f0fd3cead5a3cb04"
   },
   "source": [
    "## Let's look on the most correlated with `Target` features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a989b88c7ab448734be8dd3808281c879a3da867",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "XY = pd.concat([X,y], axis=1)\n",
    "max_corr = XY.corr()['Target'].loc[lambda x: abs(x)>0.2].index\n",
    "#min_corr = XY.corr()['Target'].loc[lambda x: abs(x)<0.05].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3e601a2ab2ed12ab539cfc48b41c135c3d5ac12b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(10,7))\n",
    "_ = sns.heatmap(XY[max_corr].corr(), vmin=-0.5, vmax=0.5, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c6e1ccce811e7a1d76282fcb8a13edf92672f834"
   },
   "source": [
    "# Model fitting\n",
    "\n",
    "We will use LightGBM classifier - LightGBM allows to build very sophysticated models with a very short training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5afc6fbb6ed16a47c0700e7ffc1d26cb2e28e778"
   },
   "source": [
    "## Use test subset for early stopping criterion\n",
    "\n",
    "This allows us to avoid overtraining and we do not need to optimise the number of trees. We also use F1 macro-averaged score to decide when to stop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "76956b6fa33cc0dcedc3602f34c4be96f6558778",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def evaluate_macroF1_lgb(truth, predictions):  \n",
    "    # this follows the discussion in https://github.com/Microsoft/LightGBM/issues/1483\n",
    "    pred_labels = predictions.reshape(len(np.unique(truth)),-1).argmax(axis=0)\n",
    "    f1 = f1_score(truth, pred_labels, average='macro')\n",
    "    return ('macroF1', f1, True) \n",
    "\n",
    "def learning_rate_power_0997(current_iter):\n",
    "    base_learning_rate = 0.1\n",
    "    min_learning_rate = 0.02\n",
    "    lr = base_learning_rate  * np.power(.99, current_iter)\n",
    "    return max(lr, min_learning_rate)\n",
    "\n",
    "import lightgbm as lgb\n",
    "fit_params={\"early_stopping_rounds\":300, \n",
    "            \"eval_metric\" : 'multiclass',\n",
    "            \"eval_metric\" : evaluate_macroF1_lgb, \n",
    "            #\"eval_set\" : [(X_train,y_train), (X_test,y_test)],\n",
    "            'eval_names': ['train', 'early_stop'],\n",
    "            'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_power_0997)],\n",
    "            'verbose': False,\n",
    "            'categorical_feature': 'auto'}\n",
    "\n",
    "#fit_params['verbose'] = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "effe8ad863afc3f72e16ca3423588cfddd13408f"
   },
   "source": [
    "# LightGBM optimal parameters\n",
    "\n",
    "The parameters are optimised with a random search in this kernel: https://www.kaggle.com/mlisovyi/lighgbm-hyperoptimisation-with-f1-macro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "22b494fee8b8880e56ce049631d7f161caef0554",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#v8\n",
    "#opt_parameters = {'colsample_bytree': 0.93, 'min_child_samples': 56, 'num_leaves': 19, 'subsample': 0.84}\n",
    "#v9\n",
    "#opt_parameters = {'colsample_bytree': 0.89, 'min_child_samples': 70, 'num_leaves': 17, 'subsample': 0.96}\n",
    "#v14\n",
    "#opt_parameters = {'colsample_bytree': 0.88, 'min_child_samples': 90, 'num_leaves': 16, 'subsample': 0.94}\n",
    "#v17\n",
    "opt_parameters = {'colsample_bytree': 0.89, 'min_child_samples': 90, 'num_leaves': 14, 'subsample': 0.96}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2dc384aeb44db2454978df78fdbb84b2b1ff3ced"
   },
   "source": [
    "# Fit a voting classifier\n",
    "Define a derived VotingClassifier class that uses pre-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "a824458abfb9f32931425d4cbe503c670dff78f6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.validation import has_fit_parameter, check_is_fitted\n",
    "\n",
    "class VotingPrefitClassifier(VotingClassifier):\n",
    "    '''\n",
    "    This implements the VotingClassifier with prefitted classifiers\n",
    "    '''\n",
    "    def fit(self, X, y, sample_weight=None, **fit_params):\n",
    "        self.estimators_ = [x[1] for x in self.estimators]\n",
    "        self.le_ = LabelEncoder().fit(y)\n",
    "        self.classes_ = self.le_.classes_\n",
    "        \n",
    "        return self    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": false,
    "_uuid": "61bd19ba9f1219a29b7ef412120ec92c070fc35c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def train_lgbm_model(X_, y_, random_state_=None, opt_parameters_={}, fit_params_={}):\n",
    "    clf  = lgb.LGBMClassifier(max_depth=-1, learning_rate=0.1, objective='multiclass',\n",
    "                             random_state=random_state_, silent=True, metric='None', \n",
    "                             n_jobs=4, n_estimators=5000, class_weight='balanced')\n",
    "    clf.set_params(**opt_parameters_)\n",
    "    return clf.fit(X_, y_, **fit_params_)\n",
    "\n",
    "# the list of classifiers for voting ensable\n",
    "clfs = []\n",
    "\n",
    "# nested CV parameters\n",
    "inner_seed = 31416\n",
    "inner_n = 10\n",
    "outer_seed = 314\n",
    "outer_n = 10\n",
    "\n",
    "# performance \n",
    "perf_eval = {'f1_oof': [],\n",
    "             'f1_ave': [],\n",
    "             'f1_std': [],\n",
    "             'f1_early_stop_ave': [],\n",
    "             'f1_early_stop': [],\n",
    "             'f1_early_stop_vc_w0_soft': [],\n",
    "             'f1_early_stop_vc_w0_hard': [],\n",
    "             'f1_early_stop_vc_w1_soft': [],\n",
    "             'f1_early_stop_vc_w1_hard': [],\n",
    "             'f1_early_stop_vc_w2_soft': [],\n",
    "             'f1_early_stop_vc_w2_hard': []\n",
    "            }\n",
    "# full-sample oof prediction\n",
    "y_full_oof = pd.Series(np.zeros(shape=(X.shape[0],)), \n",
    "                      index=X.index)\n",
    "\n",
    "outer_cv = StratifiedKFold(outer_n, shuffle=True, random_state=outer_seed)\n",
    "for n_outer_fold, (outer_trn_idx, outer_val_idx) in enumerate(outer_cv.split(X,y)):\n",
    "    print('--- Outer loop iteration: {} ---'.format(n_outer_fold))\n",
    "    X_out, y_out = X.iloc[outer_trn_idx], y.iloc[outer_trn_idx]\n",
    "    X_stp, y_stp = X.iloc[outer_val_idx], y.iloc[outer_val_idx]\n",
    "    \n",
    "    inner_cv = StratifiedKFold(inner_n, shuffle=True, random_state=inner_seed+n_outer_fold)\n",
    "    # The out-of-fold (oof) prediction for the k-1 sample in the outer CV loop\n",
    "    y_outer_oof = pd.Series(np.zeros(shape=(X_out.shape[0],)), \n",
    "                      index=X_out.index)\n",
    "    f1_scores_inner = []\n",
    "    clfs_inner = []\n",
    "    \n",
    "    for n_inner_fold, (inner_trn_idx, inner_val_idx) in enumerate(inner_cv.split(X_out,y_out)):\n",
    "        X_trn, y_trn = X_out.iloc[inner_trn_idx], y_out.iloc[inner_trn_idx]\n",
    "        X_val, y_val = X_out.iloc[inner_val_idx], y_out.iloc[inner_val_idx]\n",
    "        \n",
    "        # use _stp data for early stopping\n",
    "        fit_params[\"eval_set\"] = [(X_trn,y_trn), (X_stp,y_stp)]\n",
    "        fit_params['verbose'] = False\n",
    "        \n",
    "        clf = train_lgbm_model(X_trn, y_trn, 314+n_inner_fold, opt_parameters, fit_params)\n",
    "        \n",
    "        clfs_inner.append(('lgbm{}_inner'.format(n_outer_fold), clf))\n",
    "        # evaluate performance\n",
    "        y_outer_oof.iloc[inner_val_idx] = clf.predict(X_val)        \n",
    "        f1_scores_inner.append(f1_score(y_val, y_outer_oof.iloc[inner_val_idx], average='macro'))\n",
    "        #cleanup\n",
    "        del X_trn, y_trn, X_val, y_val\n",
    "    # Do the predictions for early-stop sub-sample for comparison with VotingPrefitClassifier\n",
    "    f1_score_inner_early_stop=[f1_score(y_stp, clf_.predict(X_stp), average='macro')\n",
    "                               for _,clf_ in clfs_inner]\n",
    "    \n",
    "    # Store performance info for this outer fold\n",
    "    perf_eval['f1_oof'].append(f1_score(y_out, y_outer_oof, average='macro'))\n",
    "    perf_eval['f1_ave'].append(np.array(f1_scores_inner).mean())\n",
    "    perf_eval['f1_std'].append(np.array(f1_scores_inner).std())\n",
    "    perf_eval['f1_early_stop_ave'].append(np.mean(f1_score_inner_early_stop))\n",
    "    # Record performance of Voting classifiers\n",
    "    w = np.array(f1_scores_inner)\n",
    "    for w_, w_name_ in [(None, '_w0'),\n",
    "                        (w/w.sum(), '_w1'),\n",
    "                        ((w**2)/np.sum(w**2), '_w2')\n",
    "                       ]:\n",
    "        vc = VotingPrefitClassifier(clfs_inner, weights=w_).fit(X_stp, y_stp)\n",
    "        vc.voting = 'soft'\n",
    "        perf_eval['f1_early_stop_vc{}_soft'.format(w_name_)].append(f1_score(y_stp, vc.predict(X_stp), average='macro'))\n",
    "        vc.voting = 'hard'\n",
    "        perf_eval['f1_early_stop_vc{}_hard'.format(w_name_)].append(f1_score(y_stp, vc.predict(X_stp), average='macro'))\n",
    "    \n",
    "    # Train main model for the voting average\n",
    "    fit_params[\"eval_set\"] = [(X_out,y_out), (X_stp,y_stp)]\n",
    "    fit_params['verbose'] = 200\n",
    "    print('Fit the final model on the outer loop iteration: ')\n",
    "    clf = train_lgbm_model(X_out, y_out, 314+n_outer_fold, opt_parameters, fit_params)\n",
    "    perf_eval['f1_early_stop'].append(f1_score(y_stp, clf.predict(X_stp), average='macro'))\n",
    "    clfs.append(('lgbm{}'.format(n_outer_fold), clf))\n",
    "    y_full_oof.iloc[outer_val_idx] = clf.predict(X_stp)\n",
    "    # cleanup\n",
    "    del inner_cv, X_out, y_out, X_stp, y_stp, clfs_inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "204a710ba8475b05516562d2e5aaadb19c7b9dd2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = np.array(perf_eval['f1_early_stop'])\n",
    "ws = [(None, '_w0'),\n",
    "      (w/w.sum(), '_w1'),\n",
    "      ((w**2)/np.sum(w**2), '_w2')\n",
    "     ]\n",
    "vc = {}\n",
    "for w_, w_name_ in ws:\n",
    "    vc['vc{}'.format(w_name_)] = VotingPrefitClassifier(clfs, weights=w_).fit(X, y)\n",
    "\n",
    "clf_final = clfs[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "241d2973497b8eb2e5214f5c8ddb76432576ba35",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_score = np.mean(perf_eval['f1_oof'])\n",
    "global_score_std = np.std(perf_eval['f1_oof'])\n",
    "\n",
    "print('Mean validation score LGBM Classifier: {:.4f}'.format(global_score))\n",
    "print('Std  validation score LGBM Classifier: {:.4f}'.format(global_score_std))\n",
    "print('EarlyStop OOF score LGBM Classifier: {:.4f}'.format(f1_score(y, y_full_oof, average='macro')))\n",
    "print('EarlyStop mean score LGBM Classifier: {:.4f}'.format(np.mean(perf_eval['f1_early_stop_ave'])))\n",
    "print('EarlyStop VotingPrefit SOFT: {:.4f}'.format(np.mean(perf_eval['f1_early_stop_vc_w0_soft'])))\n",
    "print('EarlyStop VotingPrefit HARD: {:.4f}'.format(np.mean(perf_eval['f1_early_stop_vc_w0_hard'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "27bf75112ede35a7091640c6306e9bc736d763d5"
   },
   "source": [
    "Look at the performance on invidivual folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bd04300811ce465d33b5e1e938215994857cad18",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perf_eval_df = pd.DataFrame(perf_eval)\n",
    "perf_eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "681d9bbe0fa31d443feefb52ee698975bce21e2f",
    "collapsed": true
   },
   "source": [
    "# F1 score across different classes\n",
    "Let's see if all classes show similar performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ee52eca29b70e164b286b512d940bcdcfebf870d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a71386355812dd97c74737afc7bf01b94239346e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(classification_report(y_test, clf_final.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8931a5c627a25c898f223302c50b808c10075f2f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vc.voting = 'hard'\n",
    "#print(classification_report(y_test, vc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cb4cd9c26c61dc03af31b8b62283780b3e3cc2f8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vc.voting = 'soft'\n",
    "#print(classification_report(y_test, vc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d8f28cae4872d101a3001be9d55694572e5a3cff"
   },
   "source": [
    "# Plot feature importances (using gain)\n",
    "See if added features show among most significant ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "0791014366ac43e87e4ab9c64872cf3014750f2a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_importances(feature_importance_df_, doWorst=False, n_feat=50):\n",
    "    # Plot feature importances\n",
    "    if not doWorst:\n",
    "        cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "            by=\"importance\", ascending=False)[:n_feat].index        \n",
    "    else:\n",
    "        cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "            by=\"importance\", ascending=False)[-n_feat:].index\n",
    "    \n",
    "    mean_imp = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean()\n",
    "    df_2_neglect = mean_imp[mean_imp['importance'] < 1e-3]\n",
    "    print('The list of features with 0 importance: ')\n",
    "    print(df_2_neglect.index.values.tolist())\n",
    "    del mean_imp, df_2_neglect\n",
    "    \n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "    \n",
    "    plt.figure(figsize=(8,10))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", \n",
    "                data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Features')\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('lgbm_importances.png')\n",
    "    \n",
    "importance_df = pd.DataFrame()\n",
    "importance_df[\"feature\"] = X.columns.tolist()      \n",
    "importance_df[\"importance\"] = clf_final.booster_.feature_importance('gain')\n",
    "display_importances(feature_importance_df_=importance_df, n_feat=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1ceea94109ec056012831a46ebcacbb13020eb1f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#display_importances(feature_importance_df_=importance_df, doWorst=True, n_feat=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9eaa5a0b2e0778d84f0f97784ae66132bb56b2b4"
   },
   "source": [
    "# Plot feature importances (using SHAP)\n",
    "See if added features show among most significant ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "af33296cf242c8363aa8a68561f195e7bfbe1a92",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "shap_values = shap.TreeExplainer(clf_final.booster_).shap_values(X)\n",
    "\n",
    "#shap_df = pd.DataFrame()\n",
    "#shap_df[\"feature\"] = X_train.columns.tolist()    \n",
    "#shap_df[\"importance\"] = np.sum(np.abs(shap_values), 0)[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ad7ab722c1e7f70fd7d0f1eb4d8e11e9cbd1d66c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#display_importances(feature_importance_df_=shap_df, n_feat=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c4a8e46106e456f6b0c1b7aed6a57e05fdd0095a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X, plot_type='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "78749eec7f69bcc8c587278a2c1a43ac8b5832e3"
   },
   "source": [
    "# Prepare submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "32bfd69fe130005cb88865399c460ac00c7b1574",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_subm = pd.read_csv('../input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8258f28127f235e427a25f6824566a23df61b8af",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "\n",
    "sub_file = 'submission_LGB_{:.4f}_{}.csv'.format(global_score, str(now.strftime('%Y-%m-%d-%H-%M')))\n",
    "y_subm['Target'] = clf_final.predict(test) + 1\n",
    "y_subm.to_csv(sub_file, index=False)\n",
    "\n",
    "# Store predictions with voting classifiers\n",
    "for vc_name_,vc_ in vc.items():\n",
    "    for vc_type_ in ['soft', 'hard']:\n",
    "        vc_.voting = vc_type_\n",
    "        name = '{}_{}'.format(vc_name_, vc_type_)\n",
    "        y_subm_vc = y_subm.copy(deep=True)\n",
    "        y_subm_vc.loc[:,'Target'] = vc_.predict(test) + 1\n",
    "        sub_file = 'submission_{}_LGB_{:.4f}_{}.csv'.format(name, \n",
    "                                                            global_score, \n",
    "                                                            str(now.strftime('%Y-%m-%d-%H-%M'))\n",
    "                                                           )\n",
    "        y_subm_vc.to_csv(sub_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3c6d7f945dec95a777b4221c5fe217c3eea24100",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "581b7015bd0bd5cfa37b9e503a19b0b465750567",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
