{"cells":[{"metadata":{"_uuid":"bfefc03e60a3399ee6aaec29ba71ec39bfbe83d3"},"cell_type":"markdown","source":"# Why this kernel?\nWhy should you read through this kernel? The goal is to have a visual guide on which strategy leads to the win:\n\n- the data will be read and **memory footprint will be reduced**;\n- **missing data** will be checked;\n- **aggregations of the data over teams are performed**;\n- a baseline model **on team level** will be trained:\n   - Gradient boosting model as implemented in **LightGBM** is used;\n   - **Mean absolute error (MAE) is used as the loss function** in the training (consistently with the final evaluation metric). **FAIR loss**  was also tried and seems to lead similar results\n   - Training is performed with **early stopping based on MAE metric**.\n - The training is implemented in a cross validation (CV) loop and **out-of-fold (OOF) predictions are stored** for future use in stacking.\n - **Test predictions** are obtained as an **average over predictions from models trained on k-1 fold subsets**.\n- Predictions are **clipped to `[0,1]` range**"},{"metadata":{"_uuid":"73fa2d9e1fe335a7bddc673d056b0eb092c92eca"},"cell_type":"markdown","source":"# Side note: score of 0.0411 can be achieved with only 100k games from the train set with ranking post-processing"},{"metadata":{"trusted":true,"_uuid":"de54aea187ca94ee3e3a27ebcd020cdffe855cb9"},"cell_type":"code","source":"# The number of MATCHES to use in training. Whole training dataset is used anyway. Use it to have fast turn-around. Set to 50k for all entries\nmax_matches_trn=50000\n# The number of entries from test to read in. Use it to have fast turn-around. Set to None for all entries\nmax_events_tst=None\n# Number on CV folds\nn_cv=3","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=Warning)\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8c2280c84ba4940112e24766017f2802edab801"},"cell_type":"markdown","source":"Define a function to reduce memory foorprint"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"0d144d5a4de449c1d744d528d0e1d8e21ec8899e"},"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object and col_type.name != 'category' and 'datetime' not in col_type.name:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        elif 'datetime' not in col_type.name:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4292e016577a9a1971be047e11c173462ee7d43c"},"cell_type":"markdown","source":"Read in the data"},{"metadata":{"trusted":true,"_uuid":"74fa1a7102ea9b9e660408f106db77db51284df5"},"cell_type":"code","source":"df_trn = pd.read_csv('../input/train.csv', nrows=None)\ndf_trn = reduce_mem_usage(df_trn)\n\ndf_tst = pd.read_csv('../input/test.csv',  nrows=max_events_tst)\ndf_tst = reduce_mem_usage(df_tst)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"627bbf4d3ef9d46100fb79f2fe938df8cc7c1e4a"},"cell_type":"code","source":"df_trn = df_trn.query('matchId < @max_matches_trn')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8318d9c5d82051559f33f5d9a3e48d6d3eb7cac"},"cell_type":"code","source":"print('Number of training entries after selecting a subset of matches: {}'.format(df_trn.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4e1d6766ffae52fa07f3c19cf5beb9c32ea2e28"},"cell_type":"code","source":"# we will NOT use in training\nfeatures_not2use = ['Id', 'groupId', 'matchId', 'numGroups']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5769611594f0be21b35805fbee239cc9dc72a577"},"cell_type":"markdown","source":"## How do the data look like?"},{"metadata":{"trusted":true,"_uuid":"5b34b961208d6d98839eadd4ac38c9041c70713c"},"cell_type":"code","source":"df_trn.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e52a658cfca59135a496e06224e0f4c6ed2a4de"},"cell_type":"code","source":"df_trn.info(memory_usage='deep', verbose=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d43ccdc9ca93fc1b4ff7e4d6a222a50e39577b6e"},"cell_type":"code","source":"df_tst.info(memory_usage='deep', verbose=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"29864b46f0d8fd733b14d2ed2e7809ff9137f0a3"},"cell_type":"markdown","source":"- The training dataset has 4.3M entries, which is not small and aloows for advanced models like GBM and NN to dominate.\n- The test dataset is only 1.9M entries\n- There are 25 features (+ the target in the train dataset)"},{"metadata":{"_uuid":"a71f070ac1c448815c6b8f0d78c4ee59bf66e6ab"},"cell_type":"markdown","source":"## Are there missing data?"},{"metadata":{"trusted":true,"_uuid":"de3b9626998858f27d05ce97f88b9a20a600b898"},"cell_type":"code","source":"df_trn.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7b2fb0b61e94c6d150fc07cf9410cd1e80bc873"},"cell_type":"markdown","source":"Good news: **There are no entries with `np.nan`**, so at the first glance we do not need to do anything fancy about missing data. \n\nThere might be some default values pre-filled into missing entries- this would have to be discovered."},{"metadata":{"_uuid":"8f85aa4008394e7dcf4efa4f429287807acfb1fa"},"cell_type":"markdown","source":"# Feature engineering: group by teams"},{"metadata":{"trusted":true,"_uuid":"417d40a52aba715f53298fbc6dd63eed5fd555a8"},"cell_type":"code","source":"def fe(df_):\n    for s_gb, s_name in [('groupId', 'groupSize'),\n                         ('matchId', 'numPlayers')\n                        ]:\n        series = df_.groupby(s_gb).size().to_frame()\n        series.columns = [s_name]\n        df_ = df_.merge(series, left_on=s_gb, right_index=True, how='left')\n    \n    df_aggs = df_.groupby('matchId').agg({'killPlace':['max'],\n                               'groupId': ['min']\n                              })\n    df_aggs.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in df_aggs.columns])\n    df_ = df_.merge(df_aggs, left_on='matchId', right_index=True, how='left')\n    \n    feats_add = [('rsDistance', 'rideDistance', 'swimDistance'),\n                 ('totalDistance', 'fe_rsDistance_ADD', 'walkDistance'),\n                 ('totalPoints', 'killPoints', 'winPoints'),\n                 #('', '', '')\n                ]\n    feats_sub = [('groupId_DIFF', 'groupId', 'groupId_MIN')\n                ]\n    feats_div = [\n                 ('headshotFraction', 'headshotKills', 'kills'),\n                 ('weaponsPerMeter', 'weaponsAcquired', 'fe_totalDistance_ADD'),\n                 ('kill2win', 'killPoints', 'winPoints'),\n                 ('damage2kills', 'damageDealt', 'kills'),\n                 ('killPlace_maxPlace' , 'killPlace', 'maxPlace'),\n                ('killPlace_maxKillPlace' , 'killPlace', 'killPlace_MAX'),\n                ('killPlace_numPlayers' , 'killPlace', 'numPlayers'),\n                ]\n\n    for f_new, f1, f2 in feats_add:\n        df_['fe_' + f_new + '_ADD'] = df_[f1] + df_[f2]\n    for f_new, f1, f2 in feats_sub:\n        df_['fe_' + f_new + '_SUB'] = df_[f1] - df_[f2]\n    for f_new, f1, f2 in feats_div:\n        df_['fe_' + f_new + '_DIV'] = df_[f1] / df_[f2]\n        \n    return df_\n        \ndf_trn = fe(df_trn)\ndf_tst = fe(df_tst)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e101d28e6225679a328b00c4721a47310c784304","_kg_hide-input":true},"cell_type":"code","source":"agg_team = {c: ['mean', 'min', 'max', 'median', 'sum', 'std'] \n            for c in [c \n                      for c in df_trn.columns \n                      if c not in features_not2use \n                      and c != 'winPlacePerc']\n           }\nagg_team['numGroups'] = ['size','median']\n\nprint(agg_team.keys())\n\ndef preprocess(df):    \n    df_gb = df.groupby('groupId').agg(agg_team)\n    df_gb.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in df_gb.columns])    \n    return df_gb\n\ndf_trn_gb = preprocess(df_trn)\ndf_tst_gb = preprocess(df_tst)\n\ny = df_trn.groupby('groupId')['winPlacePerc'].median()\n# target for the ranker\ny_rnk = df_trn.groupby(['matchId', 'groupId'])['winPlacePerc'].median().groupby('matchId').rank().astype(np.int16)\ny_rnk.index = y_rnk.index.droplevel(0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56576460176b2f621f29292cb8071670a2bd429e"},"cell_type":"markdown","source":"# Prepare the data"},{"metadata":{"trusted":true,"_uuid":"be3bf7e29492bcef2eacc14b79c4ae21bdea2d6e"},"cell_type":"code","source":"w_trn = df_trn_gb['numGroups_SIZE']\nw_tst = df_tst_gb['numGroups_SIZE']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c9dcfeae35694bddf63855bb39fe4b38ad13b93"},"cell_type":"code","source":"g_trn = df_trn_gb.iloc[:,:0].merge(df_trn[['matchId','groupId']].drop_duplicates().set_index('groupId'), \n                                   how='left', left_index=True, right_index=True)['matchId']\ng_tst = df_tst_gb.iloc[:,:0].merge(df_tst[['matchId','groupId']].drop_duplicates().set_index('groupId'), \n                                   how='left', left_index=True, right_index=True)['matchId']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8fe98adce4e5114c26cee619431260f57aa0c44"},"cell_type":"markdown","source":"# Train and evaluate a model\nStart by defining handy helper functions..."},{"metadata":{"trusted":true,"_uuid":"470b2148601eed76036363bac7028145954aae10","_kg_hide-input":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\nfrom sklearn.base import clone, ClassifierMixin, RegressorMixin\nimport lightgbm as lgb\n\n\ndef learning_rate_decay_power(current_iter):\n    '''\n    The function defines learning rate deay for LGBM\n    '''\n    base_learning_rate = 0.20\n    min_lr = 5e-2\n    lr = base_learning_rate  * np.power(.996, current_iter)\n    return lr if lr > min_lr else min_lr\n\n\ndef train_single_model(clf_, X_, y_, random_state_=314, opt_parameters_={}, fit_params_={}):\n    '''\n    A wrapper to train a model with particular parameters\n    '''\n    c = clone(clf_)\n    c.set_params(**opt_parameters_)\n    c.set_params(random_state=random_state_)\n    return c.fit(X_, y_, **fit_params_)\n\ndef train_model_in_CV(model, X, y, metric, metric_args={},\n                            model_name='xmodel',\n                            seed=31416, n=5,\n                            opt_parameters_={}, fit_params_={},\n                            verbose=True,\n                            groups=None, y_eval=None):\n    # the list of classifiers for voting ensable\n    clfs = []\n    # performance \n    perf_eval = {'score_i_oof': 0,\n                 'score_i_ave': 0,\n                 'score_i_std': 0,\n                 'score_i': []\n                }\n    # full-sample oof prediction\n    y_full_oof = pd.Series(np.zeros(shape=(y.shape[0],)), \n                          index=y.index)\n    \n    sample_weight=None\n    if 'sample_weight' in metric_args:\n        sample_weight=metric_args['sample_weight']\n        \n    index_weight=None\n    if 'index_weight' in metric_args:\n        index_weight=metric_args['index_weight']\n        del metric_args['index_weight']\n        \n    doSqrt=False\n    if 'sqrt' in metric_args:\n        doSqrt=True\n        del metric_args['sqrt']\n\n    if groups is None:\n        cv = KFold(n, shuffle=True, random_state=seed) #Stratified\n    else:\n        cv = GroupKFold(n)\n    # The out-of-fold (oof) prediction for the k-1 sample in the outer CV loop\n    y_oof = pd.Series(np.zeros(shape=(X.shape[0],)), \n                      index=X.index)\n    scores = []\n    clfs = []\n\n    for n_fold, (trn_idx, val_idx) in enumerate(cv.split(X, (y!=0).astype(np.int8), groups=groups)):\n        X_trn, y_trn = X.iloc[trn_idx], y.iloc[trn_idx]\n        X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n        \n        if 'LGBMRanker' in type(model).__name__ and groups is not None:\n            G_trn, G_val = groups.iloc[trn_idx], groups.iloc[val_idx]        \n\n        if fit_params_:\n            # use _stp data for early stopping\n            fit_params_[\"eval_set\"] = [(X_trn,y_trn), (X_val,y_val)]\n            fit_params_['verbose'] = verbose\n            if index_weight is not None:\n                fit_params_[\"sample_weight\"] = y_trn.index.map(index_weight).values\n                fit_params_[\"eval_sample_weight\"] = [None, y_val.index.map(index_weight).values]\n            if 'LGBMRanker' in type(model).__name__ and groups is not None:\n                fit_params_['group'] = G_trn.groupby(G_trn, sort=False).count()\n                fit_params_['eval_group'] = [G_trn.groupby(G_trn, sort=False).count(),\n                                             G_val.groupby(G_val, sort=False).count()]\n\n        #display(y_trn.head())\n        clf = train_single_model(model, X_trn, y_trn, 314+n_fold, opt_parameters_, fit_params_)\n\n        clfs.append(('{}{}'.format(model_name,n_fold), clf))\n        # oof predictions\n        if isinstance(clf, RegressorMixin):\n            y_oof.iloc[val_idx] = clf.predict(X_val)\n        elif isinstance(clf, ClassifierMixin):\n            y_oof.iloc[val_idx] = clf.predict_proba(X_val)[:,1]\n        else:\n            y_oof.iloc[val_idx] = clf.predict(X_val)\n        # prepare weights for evaluation\n        if sample_weight is not None:\n            metric_args['sample_weight'] = y_val.map(sample_weight)\n        elif index_weight is not None:\n            metric_args['sample_weight'] = y_val.index.map(index_weight).values\n        # prepare target values\n        y_true_tmp = y_val if 'LGBMRanker' not in type(model).__name__  and y_eval is None else y_eval.iloc[val_idx]\n        y_pred_tmp = y_oof.iloc[val_idx] if y_eval is None else y_oof.iloc[val_idx]        \n        #store evaluated metric\n        scores.append(metric(y_true_tmp, y_pred_tmp, **metric_args))\n        #cleanup\n        del X_trn, y_trn, X_val, y_val, y_true_tmp, y_pred_tmp\n\n    # Store performance info for this CV\n    if sample_weight is not None:\n        metric_args['sample_weight'] = y_oof.map(sample_weight)\n    elif index_weight is not None:\n        metric_args['sample_weight'] = y_oof.index.map(index_weight).values\n    perf_eval['score_i_oof'] = metric(y, y_oof, **metric_args)\n    perf_eval['score_i'] = scores\n    \n    if doSqrt:\n        for k in perf_eval.keys():\n            if 'score' in k:\n                perf_eval[k] = np.sqrt(perf_eval[k])\n        scores = np.sqrt(scores)\n            \n    perf_eval['score_i_ave'] = np.mean(scores)\n    perf_eval['score_i_std'] = np.std(scores)\n\n    return clfs, perf_eval, y_oof\n\ndef print_perf_clf(name, perf_eval):\n    print('Performance of the model:')    \n    print('Mean(Val) score inner {} Classifier: {:.4f}+-{:.4f}'.format(name, \n                                                                      perf_eval['score_i_ave'],\n                                                                      perf_eval['score_i_std']\n                                                                     ))\n    print('Min/max scores on folds: {:.4f} / {:.4f}'.format(np.min(perf_eval['score_i']),\n                                                            np.max(perf_eval['score_i'])))\n    print('OOF score inner {} Classifier: {:.4f}'.format(name, perf_eval['score_i_oof']))\n    print('Scores in individual folds: {}'.format(perf_eval['score_i']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e974b12820dbfabd78ecbeae121b704d8ff1351"},"cell_type":"code","source":"# mdl_inputs = {\n#         'lgbm1_rnk': (lgb.LGBMRanker(max_depth=-1, min_child_samples=400, random_state=314, silent=True, metric='None', \n#                                      n_jobs=4, n_estimators=5000, learning_rate=0.05,\n#                                      label_gain=np.logspace(0,100, num=101, base=2)-1\n#                                     ),\n#                  {'colsample_bytree': 0.75, 'min_child_weight': 10.0, 'num_leaves': 30, 'reg_alpha': 1, 'subsample': 0.75}, \n#                  {\"early_stopping_rounds\":100, \n#                   \"eval_metric\" : 'ndcg',\n#                   'eval_at':1,\n#                   'eval_names': ['train', 'early_stop'],\n#                   'verbose': False, \n#                   #'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_decay_power)],\n#                   'categorical_feature': 'auto'},\n#                  y_rnk\n#                 ),\n#        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"525bd2f35d449f71ae56e98ecfd35d219979a0fb"},"cell_type":"code","source":"# %%time\n# mdls = {}\n# results = {}\n# y_oofs = {}\n# for name, (mdl, mdl_pars, fit_pars, y_) in mdl_inputs.items():\n#     print('--------------- {} -----------'.format(name))\n#     mdl_, perf_eval_, y_oof_ = train_model_in_CV(mdl, df_trn_gb, y_, mean_absolute_error, \n#                                                           metric_args={},#'index_weight': w_trn},\n#                                                           model_name=name, \n#                                                           opt_parameters_=mdl_pars,\n#                                                           fit_params_=fit_pars, \n#                                                           n=n_cv,\n#                                                           verbose=500, \n#                                                           groups=g_trn, y_eval=y)\n#     results[name] = perf_eval_\n#     mdls[name] = mdl_\n#     y_oofs[name] = y_oof_\n#     print_perf_clf(name, perf_eval_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1782953ec79baf2bda4834c9c05085775cae3c4"},"cell_type":"markdown","source":"Now let's define the parameter and model in a scalable fashion (we can add later on further models to the list and it will work out-of-the-box). \n\nThe format is a dictionary with keys that are user model names and items being an array (or tuple) of:\n\n- model to be fitted;\n- additional model parameters to be set;\n- model fit parameters (they are passed to `model.fit()` call);\n- target variable."},{"metadata":{"trusted":true,"_uuid":"1ddb35cbc026b22ed8f9d7c7c11897f2a2636db9","_kg_hide-input":true},"cell_type":"code","source":"mdl_inputs = {\n        # This will be with MAE loss\n        'lgbm1_reg': (lgb.LGBMRegressor(max_depth=-1, min_child_samples=400, random_state=314, silent=True, metric='None', \n                                        n_jobs=4, n_estimators=1000, learning_rate=0.1),\n                 {'objective': 'mse', 'colsample_bytree': 0.75, 'min_child_weight': 10.0, 'num_leaves': 30, 'reg_alpha': 1},#, 'subsample': 0.75}, \n                 {\"early_stopping_rounds\":100, \n                  \"eval_metric\" : 'mae',\n                  'eval_names': ['train', 'early_stop'],\n                  'verbose': False, \n                  #'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_decay_power)],\n                  'categorical_feature': 'auto'},\n                 y,\n                 None\n                ),\n#         'lgbm1_rnk': (lgb.LGBMRanker(max_depth=-1, min_child_samples=400, random_state=314, silent=True, metric='None', \n#                                      n_jobs=4, n_estimators=5000, learning_rate=0.1,\n#                                      label_gain=np.logspace(0,100, num=101, base=2)-1\n#                                     ),\n#                  {'colsample_bytree': 0.75, 'min_child_weight': 10.0, 'num_leaves': 30, 'reg_alpha': 1, 'subsample': 0.75}, \n#                  {\"early_stopping_rounds\":100, \n#                   \"eval_metric\" : 'ndcg',\n#                   'eval_names': ['train', 'early_stop'],\n#                   'verbose': False, \n#                   #'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_decay_power)],\n#                   'categorical_feature': 'auto'},\n#                  y_rnk\n#                 ),\n       }","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb04c3bfac89f557e3c8efdcced2677bf50bd583"},"cell_type":"markdown","source":"Do the actual model training"},{"metadata":{"trusted":true,"_uuid":"71d696c729fb3e5cadeedb187a85989b631f4a8b"},"cell_type":"code","source":"%%time\nmdls = {}\nresults = {}\ny_oofs = {}\nfor name, (mdl, mdl_pars, fit_pars, y_, g_) in mdl_inputs.items():\n    print('--------------- {} -----------'.format(name))\n    mdl_, perf_eval_, y_oof_ = train_model_in_CV(mdl, df_trn_gb, y_, mean_absolute_error, \n                                                          metric_args={'index_weight': w_trn},\n                                                          model_name=name, \n                                                          opt_parameters_=mdl_pars,\n                                                          fit_params_=fit_pars, \n                                                          n=n_cv,\n                                                          verbose=500, \n                                                          groups=g_)\n    results[name] = perf_eval_\n    mdls[name] = mdl_\n    y_oofs[name] = y_oof_\n    print_perf_clf(name, perf_eval_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36901d783546db24e3a21673b33a7ca9b293623c"},"cell_type":"markdown","source":"Let's plot how predictions look like"},{"metadata":{"trusted":true,"_uuid":"87ceda22c49754e4cbc15ff53055360dbd49e2ff"},"cell_type":"code","source":"k = list(y_oofs.keys())[0]\n_ = y_oofs[k].plot('hist', bins=100, figsize=(15,6))\nplt.xlabel('Predicted winPlacePerc OOF')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"804be78620382885375e8c3a23a656614e3d545e"},"cell_type":"markdown","source":"Note, that predictions are spilled outside of the `[0,1]` range, which is not meaningful for percentage value. **We will clip test predictions to be within the meaningful range.** This will improve the score slightly"},{"metadata":{"_uuid":"3bd60837ec18d8a3037015befd4f3a0bc44f6b19"},"cell_type":"markdown","source":"## Visualise importance of features"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"426b0274b32a45f5fbdc08884d81bf04b926e807"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef display_importances(feature_importance_df_, n_feat=30, silent=False, dump_strs=[], \n                        fout_name=None, title='Features (avg over folds)'):\n    '''\n    Make a plot of most important features from a tree-based model\n\n    Parameters\n    ----------\n    feature_importance_df_ : pd.DataFrame\n        The input dataframe. \n        Must contain columns `'feature'` and `'importance'`.\n        The dataframe will be first grouped by `'feature'` and the mean `'importance'` will be calculated.\n        This allows to calculate and plot importance averaged over folds, \n        when the same features appear in the dataframe as many time as there are folds in CV.\n    n_feats : int [default: 20]\n        The maximum number of the top features to be plotted\n    silent : bool [default: False]\n        Dump additionsl information, in particular the mean importances for features \n        defined by `dump_strs` and the features with zero (<1e-3) importance\n    dump_strs : list of strings [default: []]\n        Features containing either of these srings will be printed to the screen\n    fout_name : str or None [default: None]\n        The name of the file to dump the figure. \n        If `None`, no file is created (to be used in notebooks)\n    '''\n    # Plot feature importances\n    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n            by=\"importance\", ascending=False)[:n_feat].index  \n    \n    mean_imp = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean()\n    df_2_neglect = mean_imp[mean_imp['importance'] < 1e-3]\n    \n    if not silent:\n        print('The list of features with 0 importance: ')\n        print(df_2_neglect.index.values.tolist())\n\n        pd.set_option('display.max_rows', 500)\n        pd.set_option('display.max_columns', 500)\n        for feat_prefix in dump_strs:\n            feat_names = [x for x in mean_imp.index if feat_prefix in x]\n            print(mean_imp.loc[feat_names].sort_values(by='importance', ascending=False))\n    del mean_imp, df_2_neglect\n    \n    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n    \n    plt.figure(figsize=(8,10))\n    sns.barplot(x=\"importance\", y=\"feature\", \n                data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title(title)\n    plt.tight_layout()\n\n    if fout_name is not None:\n        plt.savefig(fout_name)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1c0d180ec81be2dcfa875c0854e3e4249aeecd8"},"cell_type":"code","source":"display_importances(pd.DataFrame({'feature': df_trn_gb.columns,\n                                  'importance': mdls['lgbm1_reg'][0][1].booster_.feature_importance('gain')}),\n                    n_feat=20,\n                    title='GAIN feature importance',\n                    fout_name='feature_importance_gain.png'\n                   )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c77dec7b90de56503e3af494fbfc6f55ab4e4f6"},"cell_type":"markdown","source":"## Prepare submission"},{"metadata":{"trusted":true,"_uuid":"7bd73f0231f934fc9fc49b25f8b54a8f819e6992"},"cell_type":"code","source":"%%time\ny_subs= {}\nfor c in mdl_inputs:\n    mdls_= mdls[c]\n    y_sub = np.zeros(df_tst_gb.shape[0])\n    for mdl_ in mdls_:\n        y_sub += np.clip(mdl_[1].predict(df_tst_gb), 0, 1)\n    y_sub /= n_cv\n    \n    y_subs[c] = y_sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07bbca401b3481ab1bf8a634a62e872caa1044d3"},"cell_type":"code","source":"df_sub = pd.read_csv('../input/sample_submission.csv', nrows=max_events_tst)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b6231e9a078c84f9037b507105d8009c5d51e9b","_kg_hide-input":true},"cell_type":"code","source":"\nfor c in mdl_inputs:\n    #Submission predictions\n    y_tmp = pd.Series(y_subs[c], index=df_tst_gb.index)\n    df_sub['winPlacePerc'] = df_tst.iloc[:,:5].merge(y_tmp.to_frame(), right_index=True, left_on='groupId', how='left')[0]\n    df_sub.to_csv('sub_{}.csv'.format(c), index=False)\n    #submission predictions ranked within each game\n    y_sub_ranked = y_tmp.to_frame().merge(df_tst[['groupId', 'matchId']].drop_duplicates(), \n                           left_index=True, \n                           right_on='groupId', \n                           how='left').set_index(['matchId','groupId']).groupby(['matchId']).rank(pct=True)\n    df_sub['winPlacePerc'] = df_tst[['groupId', 'matchId']].merge(y_sub_ranked, how='left', on=['matchId','groupId'])[0]\n    df_sub.to_csv('sub_{}_ranked.csv'.format(c), index=False)\n    \n    # OOF predictions\n    oof = pd.DataFrame(index=df_trn.index, columns=['winPlacePerc'])\n    oof['winPlacePerc'] = df_trn.iloc[:,:5].merge(y_oofs[c].to_frame(), right_index=True, left_on='groupId', how='left')[0]\n    oof.clip(0, 1, inplace=True)\n    print('{} MAE OOF score = {:.4f}'.format(c, mean_absolute_error(df_trn['winPlacePerc'], oof['winPlacePerc'])))\n    oof.to_csv('oof_{}.csv'.format(c), index=False)\n    # OOF predictions ranked\n    y_oof_ranked = y_oofs[c].to_frame().merge(df_trn[['groupId', 'matchId']].drop_duplicates(), \n                           left_index=True, \n                           right_on='groupId', \n                           how='left').set_index(['matchId','groupId']).groupby(['matchId']).rank(pct=True) \n    oof['winPlacePerc'] = (df_trn[['groupId', 'matchId']].merge(y_oof_ranked, how='left', on=['matchId','groupId'])[0]).values\n    print('{} MAE OOF score = {:.4f}'.format(c, mean_absolute_error(df_trn['winPlacePerc'], oof['winPlacePerc'])))\n    oof.to_csv('oof_{}_ranked.csv'.format(c), index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb07a6de4adfcd57badea38b85ebcf7b3a2afa62"},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51cbcce916217b53bab8de9fc31060e35c4f02b6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}