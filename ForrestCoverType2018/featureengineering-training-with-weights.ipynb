{"cells":[{"metadata":{"_uuid":"616533c02d044371473a966e8fefdfacdb0f4984"},"cell_type":"markdown","source":"# Contents and goals\nThis kernel shows how to:\n\n - retreave the data;\n - preprocess the data (transforming One-Hot Encoding into Label Encoding);\n - visualise of the data in 1, 2 and 3 dimentions;\n - engineer new features inspired by the visualisation done in the previous step;\n - **use proportion of target classes in the test data in the training and model evaluation to improve consistency between local CV and LB**. This is extremely important, as it brings the class mixture in agreemnet between available training and submission datasets, thus making local performance evaluation meaningful. Many kernels in the comp either do not have model performance evaluation, or do it without weights, which leads to a gap between local and LB scores.\n - build various models on the train/test spit of the data and evaluate their performance. Hyper-parameters of the models are optimised in a dedicated kernel:  https://www.kaggle.com/mlisovyi/hyper-parameter-optimisation;\n - build voting classifiers and evaluated their performance in a nested cross-validation (CV);\n - prepare submissions\n \n Note, that the proportion of the different classes in test is: `0.37053 : 0.49681 : 0.05936 : 0.00103 : 0.01295 : 0.02687 : 0.03242`, as is discussed in https://www.kaggle.com/mlisovyi/class-fractions-in-the-test"},{"metadata":{"_uuid":"d1d5ccb306f33165b11fc23e9822f08dbd1f6fd4"},"cell_type":"markdown","source":"# General configuration and imports"},{"metadata":{"_uuid":"363c4db87adf90eae3efd54a999447350d0c0d1c","trusted":true},"cell_type":"code","source":"# controlls the number of entries to be read in by the kernel\n# can speed up processing, is a small number if set here\nmax_events = None","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D # needed for 3D scatter plots\n%matplotlib inline \nimport seaborn as sns\nimport gc\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nPATH='../input/'\n\nimport os\nprint(os.listdir(PATH))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5196b4289e9964f7ecaf0f1920a7f2fe927dd0b5"},"cell_type":"markdown","source":"# Access the data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('{}/train.csv'.format(PATH), nrows=max_events)\ntest  = pd.read_csv('{}/test.csv'.format(PATH), nrows=max_events)\n\ny = train['Cover_Type']\ntrain.drop('Cover_Type', axis=1, inplace=True)\n\ntrain.drop('Id', axis=1, inplace=True)\ntest.drop('Id', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2575bf209deaa003b39e43c95c51e907dd2953a7","trusted":true},"cell_type":"code","source":"print('Train shape: {}'.format(train.shape))\nprint('Test  shape: {}'.format(test.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bbee752f879ab4ace9c89797d29672f65363ca13","trusted":true},"cell_type":"code","source":"train.info(verbose=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0b2741050d95c1b7ec75f868c7d72028b4c5dea"},"cell_type":"markdown","source":"Is the sample imbalanced?"},{"metadata":{"_uuid":"fce9342d04c68707f533a14d3385782f7440aa9c","trusted":true},"cell_type":"code","source":"y.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76d54049c41bd77596d64986617f3ba7dfab77bc"},"cell_type":"markdown","source":"Note, that the proportion of the different classes in test is: `0.37053 : 0.49681 : 0.05936 : 0.00103 : 0.01295 : 0.02687 : 0.03242`, as is discussed in https://www.kaggle.com/mlisovyi/class-fractions-in-the-test"},{"metadata":{"_uuid":"f88d707436d0d80295e92c75bd63470e5726fd1a"},"cell_type":"markdown","source":"## OHE into LE"},{"metadata":{"_uuid":"aef656370958ac53e23a69b7f9f3bcc1b035054e"},"cell_type":"markdown","source":"Helper function to transfer One-Hot Encoding (OHE) into a Label Encoding (LE). It was taken from https://www.kaggle.com/mlisovyi/lighgbm-hyperoptimisation-with-f1-macro\n\nThe reason to convert OHE into LE is that we plan to use a tree-based model and such models are dealing well with simple interger-label encoding. Note, that this way we introduce an ordering between categories, which is not there in reality, but in practice in most use cases GBMs handle it well anyway."},{"metadata":{"_kg_hide-input":true,"_uuid":"81784b6b486bf214090eb6fbed09c88c465fdb6a","trusted":true},"cell_type":"code","source":"def convert_OHE2LE(df):\n    tmp_df = df.copy(deep=True)\n    for s_ in ['Soil_Type', 'Wilderness_Area']:\n        cols_s_ = [f_ for f_ in df.columns if f_.startswith(s_)]\n        sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n        #deal with those OHE, where there is a sum over columns == 0\n        if 0 in sum_ohe:\n            print('The OHE in {} is incomplete. A new column will be added before label encoding'\n                  .format(s_))\n            # dummy colmn name to be added\n            col_dummy = s_+'_dummy'\n            # add the column to the dataframe\n            tmp_df[col_dummy] = (tmp_df[cols_s_].sum(axis=1) == 0).astype(np.int8)\n            # add the name to the list of columns to be label-encoded\n            cols_s_.append(col_dummy)\n            # proof-check, that now the category is complete\n            sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n            if 0 in sum_ohe:\n                 print(\"The category completion did not work\")\n        tmp_df[s_ + '_LE'] = tmp_df[cols_s_].idxmax(axis=1).str.replace(s_,'').astype(np.uint16)\n        tmp_df.drop(cols_s_, axis=1, inplace=True)\n    return tmp_df\n\n\n\ndef train_test_apply_func(train_, test_, func_):\n    xx = pd.concat([train_, test_])\n    xx_func = func_(xx)\n    train_ = xx_func.iloc[:train_.shape[0], :]\n    test_  = xx_func.iloc[train_.shape[0]:, :]\n\n    del xx, xx_func\n    return train_, test_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3b296532fc774332272c125301edc78764401ff","trusted":true},"cell_type":"code","source":"train_x, test_x = train_test_apply_func(train, test, convert_OHE2LE)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5ed710b9f1715c98af4ff18d7a9b106081830b6"},"cell_type":"markdown","source":"One little caveat: looking through the OHE, `Soil_Type 7, 15`, are present in the test, but not in the training data"},{"metadata":{"_uuid":"2186f6b4d38681d35139c5f044c4198c5cb45dec"},"cell_type":"markdown","source":"The head of the training dataset"},{"metadata":{"_uuid":"fdc8e01240a6ecfe910f36d3835452c5e09904ab","trusted":true},"cell_type":"code","source":"train_x.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dda35609e2dc56484f5fd66cc92b61ebfede515d"},"cell_type":"markdown","source":"## Plot distributions of individual features"},{"metadata":{"_kg_hide-input":true,"_uuid":"4ebac2d0b7b22978fe07be0f0f99b6f14924cedc","trusted":true},"cell_type":"code","source":"train_x.hist(figsize=(16,12),bins=40)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f80ee44039e2cdfa32e9f4789535ab6e3909375"},"cell_type":"markdown","source":"Note the spike at `Hillshade_3pm==0` that was originally pointed out by @aguschin in https://github.com/aguschin/kaggle/blob/master/forestCoverType_featuresEngineering.ipynb\n\n### How do feature distributions look in the test sample?"},{"metadata":{"_kg_hide-input":true,"_uuid":"06403ba7bb10d6dbeecc4f6f2f2586802f4d2c03","trusted":true},"cell_type":"code","source":"test_x.hist(figsize=(16,12), bins=40)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3fbee5da05a6fc9349faec1a763e04bdab1689d"},"cell_type":"markdown","source":"Most distributions look similar, but note that **`Elevation` profile is totally different between train and test samples**. There are also minor differences in `Soil_Type`. \n\nThe little spike at `Hillshade_3pm==0` is also present in the test sample.\n\n## Can we reroduce the test distributions? \nHow will the training dataset look like if we apply the mixing from the test sample?"},{"metadata":{"_uuid":"ece9067e737b1489c5c473ba02b9e1f9ec41c1fe","trusted":true},"cell_type":"code","source":"test_weight_orig_map = {1: 0.37053, 2: 0.49681, 3: 0.05936, 4:0.00103, 5: 0.01295, 6: 0.02687, 7: 0.03242}\ntrain_x.hist(figsize=(16,12),bins=40, weights=y.map(test_weight_orig_map))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a4059c3b2c6cf5a40839841dbfdb48f6ab5f15d"},"cell_type":"markdown","source":"Yahoo! **Application of weights allows to reproduce the distributions of variables in the submission sample very closely.**"},{"metadata":{"_uuid":"acb32dc5fe28a114c6f5cb99ff87fa015dcea16b"},"cell_type":"markdown","source":"## Aspect shape\nLet's parametrise `Aspect` feature, which looks like a cosine function. **We will not use it, but it is fun :)**"},{"metadata":{"_kg_hide-input":true,"_uuid":"8fd7dfca62fb45e6b2d5aa8f4d38fdf06827bf34","trusted":true},"cell_type":"code","source":"# Parametrise aspect function using a simle fit closely following solution from https://stackoverflow.com/a/16716964/9640384\ny_, x_, _ = plt.hist(test_x['Aspect'], bins=30)\n\nt = x_[:-1] + np.diff(x_)/2\ndata = y_\n\nfrom scipy.optimize import leastsq\n\n#def func_aspect(x)\n\nguess_mean = np.mean(y_)\nguess_std = np.std(y_)\nguess_phase = 0\nguess_freq = np.pi/360\n\noptimize_func = lambda x: (x[0]*np.cos(x[1]*t+x[2]) + x[3] - data)/np.sqrt(data)\nest_amp, est_freq, est_phase, est_mean = leastsq(optimize_func, [guess_std, guess_freq, guess_phase, guess_mean])[0]\n\nfine_t = np.arange(0,max(t),0.1)\ndata_fit = est_amp*np.cos(est_freq*fine_t+est_phase)+est_mean\ndata_first_guess = guess_std*np.cos(guess_freq*fine_t+guess_phase) + guess_mean\n\nplt.plot(fine_t, data_first_guess, label='first guess')\nplt.plot(fine_t, data_fit, label='after fitting')\nplt.xlabel('Aspect')\nplt.legend()\nplt.show()\n\nprint('Fit parameters: \\n Amplitude = {:.1f}\\n Frequency = {:.4f}\\n Phase shift = {:.4f}\\n Pedestal = {:.1f}'.format(est_amp, est_freq, est_phase, est_mean))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94ed232d9fbfd549d10a7bd75a0885d97d1f79a2"},"cell_type":"markdown","source":"# How to impute Hillside_3pm?\nLet's follow the proposal by @jmcminis in [this post](https://www.kaggle.com/c/forest-cover-type-prediction/discussion/10693#62731):"},{"metadata":{"_kg_hide-input":true,"_uuid":"f0d3d306ee26c7afa4648a6d9165f31f3b771e09","trusted":true},"cell_type":"code","source":"def plot3D_hillshade(X_, y_, x_str='Aspect', y_str='Slope', z_str='Hillshade_3pm', figsize=(17,8)):\n    fig = plt.figure(figsize=figsize)\n    ax = plt.axes(projection='3d')\n\n    p = ax.scatter(X_[x_str], X_[y_str], X_[z_str], c=(y_ if y_ is not None else X_[z_str]))\n    _ = ax.set_xlabel(x_str)\n    _ = ax.set_ylabel(y_str)\n    _ = ax.set_zlabel(z_str)\n    plt.colorbar(p, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"9639aa36aec954e99ae8af613bf9b015cecc8389"},"cell_type":"markdown","source":"Train: sample"},{"metadata":{"_uuid":"b61d991dfff6e0743d69a7c264abe9952c3a0925","trusted":true},"cell_type":"code","source":"plot3D_hillshade(train_x, None, z_str='Hillshade_3pm')\n_ = plt.title('TRAIN sample')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50cfe30056cff5c7f58a229e1fb38f7e9948ce72"},"cell_type":"markdown","source":"Test sample:"},{"metadata":{"_kg_hide-input":true,"_uuid":"f7b8f55b2ba5133b9ea10c441b4de100674f30ac","trusted":true},"cell_type":"code","source":"plot3D_hillshade(test_x, None, z_str='Hillshade_3pm')\n_ = plt.title('TEST sample')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"028d54c2e7621a5e059ad813d046d690cd192478"},"cell_type":"markdown","source":"We see that the location of the `Hillshade_3pm==0` events is the same between train and test samples. The best way would be to parametrise `Hillshade_3pm` as a function of `Aspect` and `Slope`. But this requires parametric form, that is not trivial. \n\nThe simpler solution is to **run KNN algorithm and assign the `Hillshade_3pm` values for these tricky events  using `Aspect` and `Slope` as inputs**. Note, that one would want to do it on the data subset used for training only and predict with such learned model for the test/validation subset as well as for the submission dataset. The same applies for a CV loop\n\nThe following suits only as an illustration and proof-of-consept"},{"metadata":{"_kg_hide-input":true,"_uuid":"b09120e8fabfcb91fef051cd0c6b84ca83d5d549","trusted":true},"cell_type":"code","source":"# define a grid for visualisation\nx_aspect = np.linspace(0, 360, 360)\nx_slope  = np.linspace(0, 60, 60)\ngrid_aspect, grid_slope = np.meshgrid(x_aspect, x_slope)\ngrid_aspect = grid_aspect.ravel()\ngrid_slope = grid_slope.ravel()\n\n# a simplified version of `plot3D_hillshade` function\ndef plot3D_basic(x, y, z, figsize=(17,8)):\n    fig = plt.figure(figsize=figsize)\n    ax = plt.axes(projection='3d')\n    p = ax.scatter(x, y, z, c=z)\n    plt.colorbar(p, ax=ax)\n\n# select the training data with non-zero `Hillshade_3pm`\ntrain_nonzero_3pm = train_x.query('Hillshade_3pm >= 1')\n    \n# train a KNN model on the full train set for illustration purpose only\nfrom sklearn.neighbors import KNeighborsRegressor\nk=100\nknn = KNeighborsRegressor(n_neighbors=k).fit(train_nonzero_3pm[['Aspect', 'Slope']], \n                                             train_nonzero_3pm['Hillshade_3pm'])\n#predict on the predefined grid for plotting\npreds_3pm = knn.predict(pd.DataFrame({'Aspect': grid_aspect,\n                                      'Slope':  grid_slope}))\n#Do the final plot\nplot3D_basic(grid_aspect, grid_slope, preds_3pm)\n_ = plt.title('KNN: {}'.format(k))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"888fa92feb15f2cb6810af27769533be5f43ff2f"},"cell_type":"markdown","source":"# How do other Hillside_ variables look like?"},{"metadata":{"_uuid":"bb3c74fa34475ceb0b4702caf93c0345ebb7ea89","trusted":true},"cell_type":"code","source":"plot3D_hillshade(train_x, None, z_str='Hillshade_Noon')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c0c48aa313eb8b703979fc45b192476c61ab584a","trusted":true},"cell_type":"code","source":"plot3D_hillshade(train_x, None, z_str='Hillshade_9am')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23b8feb881fb7277161585567be04b4cbc83b02a","trusted":true},"cell_type":"code","source":"plot3D_hillshade(train_x, None, x_str='Hillshade_9am', y_str='Hillshade_Noon', z_str='Hillshade_3pm')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41ae11ae8ff4d4a61dad34bef8e8e812a97ed572"},"cell_type":"markdown","source":"## Interactive 3D plots\nAt the moment `cufflinks` do not support `plotly>=3.0.0` (see [issue #119 on github](https://github.com/santosjorge/cufflinks/issues/119)) and kaggle docker image has `plotly==3.1.1`. So cufflinks are useless.  \n\nWriting directly in plotly is combersome, so this section is commented out for the time being"},{"metadata":{"_uuid":"cca41b2a33c4780f7aee161e6bb763b339fc5b30","trusted":true},"cell_type":"code","source":"# import plotly\n# import plotly.plotly as py\n# import plotly.graph_objs as go\n# plotly.offline.init_notebook_mode(connected=True)\n# import cufflinks as cf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"95d42a69127a6c7ea1e39bc8a426dcfa716db4fd","trusted":true},"cell_type":"code","source":"# train_x.iplot(kind='scatter3D', x='Aspect', y='Slope', z='Hillshade_3pm', bgcolor='yellowgreen')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58f3193790d661de5a7d430c340a21fb9167a7fe","trusted":true},"cell_type":"code","source":"#plotly.__version__","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"405e6497f971fb25ed7e9815ecdcceb453faf475"},"cell_type":"markdown","source":"# EDA\nThe scatter-plot implementation and visualisation come from https://github.com/aguschin/kaggle/blob/master/forestCoverType_featuresEngineering.ipynb. The plotting function will highlight different target classes with different colours."},{"metadata":{"_kg_hide-input":true,"_uuid":"aa8fb71bfb8bb1c5bc3e0f8b48db6e66bd3702d6","trusted":true},"cell_type":"code","source":"def plotc(c1, c2, labels, size=10, doGrid=True):\n    fig = plt.figure(figsize=(16,8))\n\n    plt.scatter(c1, c2, c=labels.values, s=size)\n    plt.colorbar()\n    plt.xlabel(c1.name)\n    plt.ylabel(c2.name)\n    if doGrid:\n        plt.grid(True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e1a76de5e1195abc79d828d1fa7f8d2508d35b2","scrolled":false,"trusted":true},"cell_type":"code","source":"plotc(train_x['Vertical_Distance_To_Hydrology'],  train_x['Elevation'], y, size=10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3b92b2a0d2d14b184970af1c161940d9e30b758"},"cell_type":"markdown","source":"These inclined structures indicate a linear dependence between the two variables. However, such linear dependence is hard to capture with decision trees, so one typicaly introduces linear combinations of such features"},{"metadata":{"_uuid":"e102b8d189652c8078bd46bc577bd7f2b54de994","trusted":true},"cell_type":"code","source":"plotc(train_x['Vertical_Distance_To_Hydrology'], train_x['Elevation']-train_x['Vertical_Distance_To_Hydrology'], y, size=10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ace28d00aa82053bd677a1f350abb0e35568744e"},"cell_type":"markdown","source":"With such new feature it will be much easier for a decision tree to separate between classes with cuts on the X axis. \nThis shows the importance of the difference variables"},{"metadata":{"_uuid":"503c8afca4e50731cb542ac11df37e211154679b","trusted":true},"cell_type":"code","source":"plotc(train_x['Horizontal_Distance_To_Hydrology'], train_x['Elevation'],   y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0eeacc1d2936f1eebfc07de87bda91f70084a55"},"cell_type":"markdown","source":"Here there is also a linear dependence, but it is weaker than what we saw before. We use the linear combination developed by @aguschin:"},{"metadata":{"_uuid":"e29da7f44942437f833c2d0a8f87352a94bec41e","trusted":true},"cell_type":"code","source":"plotc(train_x['Horizontal_Distance_To_Hydrology'], train_x['Elevation']- train_x['Horizontal_Distance_To_Hydrology']*0.2,  y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aed67336822ef1348f33ba4080c92056375ae994"},"cell_type":"markdown","source":"# Let's do actual Feature engineering"},{"metadata":{"_kg_hide-input":true,"_uuid":"941935f4e9e5eed0c25211f481ffc0bdc5f99cdf","trusted":true},"cell_type":"code","source":"def preprocess(df_):\n    #df_.drop('Elevation', axis=1, inplace=True)\n    df_['fe_E_Min_02HDtH'] = df_['Elevation']- df_['Horizontal_Distance_To_Hydrology']*0.2\n    df_['fe_Distance_To_Hydrology'] = np.sqrt(df_['Horizontal_Distance_To_Hydrology']**2 + \n                                              df_['Vertical_Distance_To_Hydrology']**2)\n    \n    feats_sub = [('E_Min_VDtH', 'Elevation', 'Vertical_Distance_To_Hydrology'),\n                 ('HD_Hydrology_Min_Roadways', 'Horizontal_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways'),\n                 ('HD_Hydrology_Min_Fire', 'Horizontal_Distance_To_Hydrology', 'Horizontal_Distance_To_Fire_Points'),\n                 ('Hillshade_9am_Min_Noon', 'Hillshade_9am', 'Hillshade_Noon'),\n                 ('Hillshade_Noon_Min_3pm', 'Hillshade_Noon', 'Hillshade_3pm'),\n                 ('Hillshade_9am_Min_3pm', 'Hillshade_9am', 'Hillshade_3pm')\n                ]\n    feats_add = [('E_Add_VDtH', 'Elevation', 'Vertical_Distance_To_Hydrology'),\n                 ('HD_Hydrology_Add_Roadways', 'Horizontal_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways'),\n                 ('HD_Hydrology_Add_Fire', 'Horizontal_Distance_To_Hydrology', 'Horizontal_Distance_To_Fire_Points'),\n                 ('Hillshade_9am_Add_Noon', 'Hillshade_9am', 'Hillshade_Noon'),\n                 ('Hillshade_Noon_Add_3pm', 'Hillshade_Noon', 'Hillshade_3pm'),\n                 ('Hillshade_9am_Add_3pm', 'Hillshade_9am', 'Hillshade_3pm')\n                ]\n    \n    for f_new, f1, f2 in feats_sub:\n        df_['fe_' + f_new] = df_[f1] - df_[f2]\n    for f_new, f1, f2 in feats_add:\n        df_['fe_' + f_new] = df_[f1] + df_[f2]\n        \n    df_['fe_Hillshade_Mean'] = (df_['Hillshade_9am'] + df_['Hillshade_Noon'] + df_['Hillshade_3pm'])/3\n    df_['fe_Hillshade_Mean_Div_E'] = (df_['fe_Hillshade_Mean'] / df_['Elevation']).clip(upper=255)\n    df_['fe_Hillshade_Mean_Div_Aspect'] = (df_['fe_Hillshade_Mean'] / df_['Aspect']).clip(upper=255)\n    \n    # A few composite variables\n    df_['fe_Hillshade_Ratio1'] = (df_['fe_Hillshade_9am_Min_Noon'] / df_['fe_Hillshade_Noon_Min_3pm']).clip(lower=-5, upper=2)\n    df_['fe_Hillshade_Ratio2'] = (df_['fe_Hillshade_9am_Min_3pm']  / df_['Hillshade_Noon']).clip(lower=-2, upper=2)\n        \n    # The feature is advertised in https://douglas-fraser.com/forest_cover_management.pdf\n    df_['fe_Shade9_Mul_VDtH'] = df_['Hillshade_9am'] * df_['Vertical_Distance_To_Hydrology']\n    \n    # Features inherited from https://www.kaggle.com/leannelong3/r-random-forest\n    df_['Elevation_bins50'] = np.floor_divide(df_['Elevation'], 50)\n    df_['fe_Horizontal_Distance_To_Roadways_Log'] = np.log1p(df_['Horizontal_Distance_To_Roadways'])\n\n    # this mapping comes from https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.info\n    climatic_zone = {}\n    geologic_zone = {}\n    for i in range(1,41):\n        if i <= 6:\n            climatic_zone[i] = 2\n            geologic_zone[i] = 7\n        elif i <= 8:\n            climatic_zone[i] = 3\n            geologic_zone[i] = 5\n        elif i == 9:\n            climatic_zone[i] = 4\n            geologic_zone[i] = 2\n        elif i <= 13:\n            climatic_zone[i] = 4\n            geologic_zone[i] = 7\n        elif i <= 15:\n            climatic_zone[i] = 5\n            geologic_zone[i] = 1\n        elif i <= 17:\n            climatic_zone[i] = 6\n            geologic_zone[i] = 1\n        elif i == 18:\n            climatic_zone[i] = 6\n            geologic_zone[i] = 7\n        elif i <= 21:\n            climatic_zone[i] = 7\n            geologic_zone[i] = 1\n        elif i <= 23:\n            climatic_zone[i] = 7\n            geologic_zone[i] = 2\n        elif i <= 34:\n            climatic_zone[i] = 7\n            geologic_zone[i] = 7\n        else:\n            climatic_zone[i] = 8\n            geologic_zone[i] = 7\n            \n    df_['Climatic_zone_LE'] = df_['Soil_Type_LE'].map(climatic_zone).astype(np.uint8)\n    df_['Geologic_zone_LE'] = df_['Soil_Type_LE'].map(geologic_zone).astype(np.uint8)\n    \n    for c in df_.columns:\n        if c.startswith('fe_'):\n            df_[c] = df_[c].astype(np.float32)\n    return df_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42c3309fcabc6b481984a2e03509b383698392af","trusted":true},"cell_type":"code","source":"train_x = preprocess(train_x)\ntest_x = preprocess(test_x)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0d4e6071e23750d7804db00fb83316651f87ffb"},"cell_type":"markdown","source":"# Train various classifiers"},{"metadata":{"_uuid":"03b46c6db6ce788be24e9ecba73afd15362beed9","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom  sklearn.linear_model import LogisticRegression\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0736858eadff87e723063d84a51a90edb3f17ebc"},"cell_type":"markdown","source":"Some of the models (LightGBM in particular) expect the labels to start from 0"},{"metadata":{"_uuid":"5c712a0b7adb411215207ef4c126934efa40fb85","trusted":true},"cell_type":"code","source":"y = y-1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49a4f86863b87ead420f2556fc98f9cf06e180ff"},"cell_type":"markdown","source":"The train/test split is used in simple model training"},{"metadata":{"_uuid":"f1d406d0588bf959c4c76e1057db1acfbe576188","trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(train_x, y, test_size=0.15, random_state=315, stratify=None)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db49c17a98929c70511d8fd7ba9aa0823ffecc31"},"cell_type":"markdown","source":"These are the weights that correspond to the test/submission class mixture. using these will allow to adjust importance of mistakes on different classes"},{"metadata":{"_uuid":"f65eddad4cb19c11b8de0dbdb6494f2063f5969d","trusted":true},"cell_type":"code","source":"test_weight_map = {0: 0.37053, 1: 0.49681, 2: 0.05936, 3:0.00103, 4: 0.01295, 5: 0.02687,6 : 0.03242}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7aedbb5646be5574a003d46faef12bb7aed91300"},"cell_type":"markdown","source":"Various machine-learning models to be used to model the data"},{"metadata":{"trusted":true,"_uuid":"ced6a0c0cad96e56e0a12b4aecb875579d8e9fbf"},"cell_type":"code","source":"train_x.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a97420c99db2876bcd3af3966fcc0357cda9d46","trusted":true},"cell_type":"code","source":"def learning_rate_decay_power_0995(current_iter):\n    base_learning_rate = 0.15\n    lr = base_learning_rate  * np.power(.995, current_iter)\n    return lr if lr > 1e-2 else 1e-2\n\nclf_inputs = {'rf': (RandomForestClassifier(n_estimators=200, max_depth=1, random_state=314, n_jobs=4),\n               {'max_depth': 30}, \n               {}),\n        'rf_test': (RandomForestClassifier(n_estimators=200, max_depth=1, random_state=314, n_jobs=4,\n                                           class_weight=test_weight_map),\n               {'max_depth': 30}, \n               {}),\n        'xt400': (ExtraTreesClassifier(n_estimators=400, max_depth=1, max_features='auto',random_state=314, n_jobs=4,\n                                       criterion='entropy'),\n               {'max_depth': 25},\n               {}),\n        'xt400_test': (ExtraTreesClassifier(n_estimators=400, max_depth=1, max_features='auto',random_state=314, n_jobs=4,\n                                            class_weight=test_weight_map, criterion='entropy'),\n               {'max_depth': 25},\n               {}),\n        'lgbm': (lgb.LGBMClassifier(max_depth=-1, min_child_samples=400, \n                                 random_state=314, silent=True, metric='None', \n                                 n_jobs=4, n_estimators=5000, learning_rate=0.1), \n                 {'loss': 'multiclass', 'colsample_bytree': 0.75, 'min_child_weight': 1, 'num_leaves': 20, 'subsample': 0.75}, \n                 {'eval_set': [(X_train, y_train), (X_test, y_test)], \n                  'eval_names': ['train', 'early_stop'],\n                  'eval_metric': 'multi_error', 'verbose':500, 'early_stopping_rounds':100, \n                  'callbacks':[lgb.reset_parameter(learning_rate=learning_rate_decay_power_0995)]}\n                ),\n        'lgbm_test': (lgb.LGBMClassifier(max_depth=-1, min_child_samples=400, \n                                 random_state=314, silent=True, metric='None', \n                                 n_jobs=4, n_estimators=5000, learning_rate=0.1,\n                                        class_weight=test_weight_map), \n                 {'loss': 'multiclass', 'colsample_bytree': 0.75, 'min_child_weight': 1, 'num_leaves': 20, 'subsample': 0.75}, \n                 {'eval_set': [(X_train, y_train), (X_test, y_test)], \n                  'eval_names': ['train', 'early_stop'],\n                  'eval_class_weight': [{}, test_weight_map],\n                  'eval_metric': 'multi_error', 'verbose':500, 'early_stopping_rounds':100, \n                  'callbacks':[lgb.reset_parameter(learning_rate=learning_rate_decay_power_0995)]}\n                )\n       }","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ba7fa403dabfbfb7f340922fd20ec5dcc008d59","trusted":true},"cell_type":"code","source":"clfs = {}\nfor name, (clf, clf_pars, fit_pars) in clf_inputs.items():\n    print('--------------- {} -----------'.format(name))\n    clf.set_params(**clf_pars)\n    clf = clf.fit(X_train, y_train, **fit_pars)\n    acc_pars={}\n    print('{}: with/without weight  train = {:.4f}/{:.4f}, test = {:.4f}/{:.4f}'.format(name,\n        accuracy_score(y_train, clf.predict(X_train), sample_weight=y_train.map(test_weight_map)),\n        accuracy_score(y_train, clf.predict(X_train)),\n        accuracy_score(y_test,  clf.predict(X_test), sample_weight=y_test.map(test_weight_map)),\n        accuracy_score(y_test,  clf.predict(X_test))\n                                                     )\n         )\n    clfs[name] = clf.fit(train_x, y, **fit_pars)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3afbfd0373c1e4a3363a8661e8b0d4430222257"},"cell_type":"markdown","source":"## Feature importances in the tree models\nNote that all models report different order of importance"},{"metadata":{"_kg_hide-input":true,"_uuid":"b8b0096950c4faff1dbbfaffab1c38b1e1a42fdb","trusted":true},"cell_type":"code","source":"def display_importances(feature_importance_df_, n_feat=30, silent=False, dump_strs=[], fout_name=None):\n    '''\n    Make a plot of most important features from a tree-based model\n\n    Parameters\n    ----------\n    feature_importance_df_ : pd.DataFrame\n        The input dataframe. \n        Must contain columns `'feature'` and `'importance'`.\n        The dataframe will be first grouped by `'feature'` and the mean `'importance'` will be calculated.\n        This allows to calculate and plot importance averaged over folds, \n        when the same features appear in the dataframe as many time as there are folds in CV.\n    n_feats : int [default: 20]\n        The maximum number of the top features to be plotted\n    silent : bool [default: False]\n        Dump additionsl information, in particular the mean importances for features \n        defined by `dump_strs` and the features with zero (<1e-3) importance\n    dump_strs : list of strings [default: []]\n        Features containing either of these srings will be printed to the screen\n    fout_name : str or None [default: None]\n        The name of the file to dump the figure. \n        If `None`, no file is created (to be used in notebooks)\n    '''\n    # Plot feature importances\n    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n            by=\"importance\", ascending=False)[:n_feat].index  \n    \n    mean_imp = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean()\n    df_2_neglect = mean_imp[mean_imp['importance'] < 1e-3]\n    \n    if not silent:\n        print('The list of features with 0 importance: ')\n        print(df_2_neglect.index.values.tolist())\n\n        pd.set_option('display.max_rows', 500)\n        pd.set_option('display.max_columns', 500)\n        for feat_prefix in dump_strs:\n            feat_names = [x for x in mean_imp.index if feat_prefix in x]\n            print(mean_imp.loc[feat_names].sort_values(by='importance', ascending=False))\n    del mean_imp, df_2_neglect\n    \n    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n    \n    plt.figure(figsize=(8,10))\n    sns.barplot(x=\"importance\", y=\"feature\", \n                data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('Features (avg over folds)')\n    plt.tight_layout()\n\n    if fout_name is not None:\n        plt.savefig(fout_name)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e231720b332efd087885cb7aa9d497eeecfbb16","scrolled":false,"trusted":true},"cell_type":"code","source":"display_importances(pd.DataFrame({'feature': train_x.columns,\n                                  'importance': clfs['xt400_test'].feature_importances_}),\n                    silent=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"5240defd63826072d64a307f4e1e71529cf6f2f6","trusted":true},"cell_type":"code","source":"display_importances(pd.DataFrame({'feature': train_x.columns,\n                                  'importance': clfs['xt400'].feature_importances_}),\n                    silent=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"476aa681b36278c3c0f4807a90a874b22fd905cb","trusted":true},"cell_type":"code","source":"display_importances(pd.DataFrame({'feature': train_x.columns,\n                                  'importance': clfs['rf_test'].feature_importances_}),\n                    silent=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"79484a7f4637cabeb5f36db95c6edf89ebd9e18a","trusted":true},"cell_type":"code","source":"display_importances(pd.DataFrame({'feature': train_x.columns,\n                                  'importance': clfs['rf'].feature_importances_}),\n                    silent=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79cbe23e8e5515fab27a88a95af6cd299cd3d89b","trusted":true},"cell_type":"code","source":"display_importances(pd.DataFrame({'feature': train_x.columns,\n                                  'importance': clfs['lgbm_test'].feature_importances_}),\n                    silent=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"159442425be1370d50a182da5280a312c7008cac","trusted":true},"cell_type":"code","source":"display_importances(pd.DataFrame({'feature': train_x.columns,\n                                  'importance': clfs['lgbm'].feature_importances_}),\n                    silent=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"7d5f7c513a6cfe57891475f0db86669a31950986","trusted":true},"cell_type":"code","source":"display_importances(pd.DataFrame({'feature': train_x.columns,\n                                  'importance': clfs['lgbm_test'].booster_.feature_importance('gain')}),\n                    silent=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b88add385bce7d7d48b43d37ecba58e7dcf9417"},"cell_type":"markdown","source":"# Feature importance with SHAP\n"},{"metadata":{"_uuid":"d5364434e1d8493b791b50ccab9f16d30859ef71","trusted":true},"cell_type":"code","source":"import shap\nshap_values = shap.TreeExplainer(clfs['lgbm'].booster_).shap_values(X_test)\nshap.summary_plot(shap_values, X_test, plot_type='bar')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86d9caa711cdd9b89fb3976520c9e44fab88da5f"},"cell_type":"markdown","source":"# Voting classifier with nested CV evaluation"},{"metadata":{"_kg_hide-input":true,"_uuid":"cc9c4f3dc9b66791ca517137531cee1fb1aa4395","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.preprocessing import LabelEncoder\n\nclass VotingPrefitClassifier(VotingClassifier):\n    '''\n    This implements the VotingClassifier with prefitted classifiers\n    '''\n    def fit(self, X, y, sample_weight=None, **fit_params):\n        self.estimators_ = [x[1] for x in self.estimators]\n        self.le_ = LabelEncoder().fit(y)\n        self.classes_ = self.le_.classes_\n        \n        return self","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cae8d94a12eb02b8f54af1e3b8c5b7d87d063b04","trusted":true},"cell_type":"code","source":"# clf_inputs = {\n#         'xt200_test': (ExtraTreesClassifier(n_estimators=200, max_depth=1, max_features='auto',random_state=314, n_jobs=4,\n#                                             class_weight=test_weight_map),\n#                {'max_depth': 25},\n#                {})\n#        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df34e5ccc15bb8c828f1a3d3bab3bd55706eabff"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"ec89c3b6241b26625ba17fa4ff3ec205d1cace95","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.base import clone\n\ndef train_single_model(clf_, X_, y_, random_state_=314, opt_parameters_={}, fit_params_={}):\n    c = clone(clf_)\n    c.set_params(**opt_parameters_)\n    c.set_params(random_state=random_state_)\n    return c.fit(X_, y_, **fit_params_)\n\ndef train_model_in_nestedCV(model, X, y, metric, metric_args={},\n                            model_name='xmodel',\n                            inner_seed=31416, inner_n=10, outer_seed=314, outer_n=10,\n                            opt_parameters_={}, fit_params_={},\n                            verbose=True):\n    # the list of classifiers for voting ensable\n    clfs = []\n    # performance \n    perf_eval = {'score_i_oof': [],\n                 'score_i_ave': [],\n                 'score_i_std': [],\n                 'score_i_early_stop_ave': [],\n                 'score_o_early_stop': [],\n                 'score_o_early_stop_vc_w0_soft': [],\n                 'score_o_early_stop_vc_w0_hard': []\n                }\n    # full-sample oof prediction\n    y_full_oof = pd.Series(np.zeros(shape=(y.shape[0],)), \n                          index=y.index)\n    \n    if 'sample_weight' in metric_args:\n        sample_weight=metric_args['sample_weight']\n\n    outer_cv = StratifiedKFold(outer_n, shuffle=True, random_state=outer_seed)\n    for n_outer_fold, (outer_trn_idx, outer_val_idx) in enumerate(outer_cv.split(X,y)):\n        print('--- Outer loop iteration: {} ---'.format(n_outer_fold))\n        X_out, y_out = X.iloc[outer_trn_idx], y.iloc[outer_trn_idx]\n        X_stp, y_stp = X.iloc[outer_val_idx], y.iloc[outer_val_idx]\n\n        inner_cv = StratifiedKFold(inner_n, shuffle=True, random_state=inner_seed+n_outer_fold)\n        # The out-of-fold (oof) prediction for the k-1 sample in the outer CV loop\n        y_outer_oof = pd.Series(np.zeros(shape=(X_out.shape[0],)), \n                          index=X_out.index)\n        scores_inner = []\n        clfs_inner = []\n\n        for n_inner_fold, (inner_trn_idx, inner_val_idx) in enumerate(inner_cv.split(X_out,y_out)):\n            X_trn, y_trn = X_out.iloc[inner_trn_idx], y_out.iloc[inner_trn_idx]\n            X_val, y_val = X_out.iloc[inner_val_idx], y_out.iloc[inner_val_idx]\n\n            if fit_params_:\n                # use _stp data for early stopping\n                fit_params_[\"eval_set\"] = [(X_trn,y_trn), (X_stp,y_stp)]\n                fit_params_['verbose'] = False\n\n            clf = train_single_model(model, X_trn, y_trn, 314+n_inner_fold, opt_parameters_, fit_params_)\n\n            clfs_inner.append(('{}{}_inner'.format(model_name,n_inner_fold), clf))\n            # evaluate performance\n            y_outer_oof.iloc[inner_val_idx] = clf.predict(X_val)\n            if 'sample_weight' in metric_args:\n                metric_args['sample_weight'] = y_val.map(sample_weight)\n            scores_inner.append(metric(y_val, y_outer_oof.iloc[inner_val_idx], **metric_args))\n            #cleanup\n            del X_trn, y_trn, X_val, y_val\n\n        # Store performance info for this outer fold\n        if 'sample_weight' in metric_args:\n            metric_args['sample_weight'] = y_outer_oof.map(sample_weight)\n        perf_eval['score_i_oof'].append(metric(y_out, y_outer_oof, **metric_args))\n        perf_eval['score_i_ave'].append(np.mean(scores_inner))\n        perf_eval['score_i_std'].append(np.std(scores_inner))\n        \n        # Do the predictions for early-stop sub-sample for comparison with VotingPrefitClassifier\n        if 'sample_weight' in metric_args:\n            metric_args['sample_weight'] = y_stp.map(sample_weight)\n        score_inner_early_stop = [metric(y_stp, clf_.predict(X_stp), **metric_args)\n                                   for _,clf_ in clfs_inner]\n        perf_eval['score_i_early_stop_ave'].append(np.mean(score_inner_early_stop))\n        \n        # Record performance of Voting classifiers\n        w = np.array(scores_inner)\n        for w_, w_name_ in [(None, '_w0')#,\n                            #(w/w.sum(), '_w1'),\n                            #((w**2)/np.sum(w**2), '_w2')\n                           ]:\n            vc = VotingPrefitClassifier(clfs_inner, weights=w_).fit(X_stp, y_stp)\n            for vote_type in ['soft', 'hard']:\n                vc.voting = vote_type\n                if 'sample_weight' in metric_args:\n                    metric_args['sample_weight'] = y_stp.map(sample_weight)\n                perf_eval['score_o_early_stop_vc{}_{}'.format(w_name_, vote_type)].append(metric(y_stp, vc.predict(X_stp), **metric_args))\n\n        if fit_params_:\n            # Train main model for the voting average\n            fit_params_[\"eval_set\"] = [(X_out,y_out), (X_stp,y_stp)]\n            if verbose:\n                fit_params_['verbose'] = 200\n        #print('Fit the final model on the outer loop iteration: ')\n        clf = train_single_model(model, X_out, y_out, 314+n_outer_fold, opt_parameters_, fit_params_)\n        if 'sample_weight' in metric_args:\n            metric_args['sample_weight'] = y_stp.map(sample_weight)\n        perf_eval['score_o_early_stop'].append(metric(y_stp, clf.predict(X_stp), **metric_args))\n        clfs.append(('{}{}'.format(model_name,n_outer_fold), clf))\n        y_full_oof.iloc[outer_val_idx] = clf.predict(X_stp)\n        # cleanup\n        del inner_cv, X_out, y_out, X_stp, y_stp, clfs_inner\n\n    return clfs, perf_eval, y_full_oof\n\ndef print_nested_perf_clf(name, perf_eval):\n    print('Performance of the inner-loop model (the two should agree):')\n    print('  Mean(mean(Val)) score inner {} Classifier: {:.4f}+-{:.4f}'.format(name, \n                                                                      np.mean(perf_eval['score_i_ave']),\n                                                                      np.std(perf_eval['score_i_ave'])\n                                                                     ))\n    print('  Mean(mean(EarlyStop)) score inner {} Classifier: {:.4f}+-{:.4f}'.format(name, \n                                                                      np.mean(perf_eval['score_i_early_stop_ave']),\n                                                                      np.std(perf_eval['score_i_early_stop_ave'])\n                                                                     ))\n    print('Mean(inner OOF) score inner {} Classifier: {:.4f}+-{:.4f}'.format(name, \n                                                                       np.mean(perf_eval['score_i_oof']), \n                                                                       np.std(perf_eval['score_i_oof'])\n                                                                      ))\n    print('Mean(EarlyStop) score outer {} Classifier: {:.4f}+-{:.4f}'.format(name, \n                                                                      np.mean(perf_eval['score_o_early_stop']),\n                                                                      np.std(perf_eval['score_o_early_stop'])\n                                                                     ))\n    print('Mean(EarlyStop) outer VotingPrefit SOFT: {:.4f}+-{:.4f}'.format(np.mean(perf_eval['score_o_early_stop_vc_w0_soft']),\n                                                                           np.std(perf_eval['score_o_early_stop_vc_w0_soft'])                                                                    \n                                                                    ))\n    print('Mean(EarlyStop) outer VotingPrefit HARD: {:.4f}+-{:.4f}'.format(np.mean(perf_eval['score_o_early_stop_vc_w0_hard']),\n                                                                           np.std(perf_eval['score_o_early_stop_vc_w0_hard'])\n                                                                    ))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"813738dcdb3b94d86f8593f1e642cef5ebcb1a98","scrolled":false,"trusted":true},"cell_type":"code","source":"for k in [name for name in clf_inputs if '_test' not in name]:\n    del clf_inputs[k]\n\nclfss = {}\nvcs = {}\nresults = {}\nfor name, (clf, clf_pars, fit_pars) in clf_inputs.items():\n    print('--------------- {} -----------'.format(name))\n    clfs_, perf_eval, y_full_oof = train_model_in_nestedCV(clf, train_x, y, accuracy_score, \n                                                          metric_args={} if '_test' not in name else {'sample_weight': test_weight_map},\n                                                          model_name=name, \n                                                          opt_parameters_=clf_pars,\n                                                          fit_params_=fit_pars, \n                                                          inner_n=10, outer_n=10,\n                                                          verbose=False)\n    results[name] = perf_eval\n    clfss[name] = clfs_\n    ws = [(None, '_w0')#,\n      #(w/w.sum(), '_w1'),\n      #((w**2)/np.sum(w**2), '_w2')\n     ]\n    vcs[name] = {}\n    for w_, w_name_ in ws:\n        vcs[name]['vc{}'.format(w_name_)] = VotingPrefitClassifier(clfs_, weights=w_).fit(train_x, y)\n\n    print_nested_perf_clf(name, perf_eval)\n    print('Outer OOF score {} Classifier: {:.4f}'.format(name, accuracy_score(y, y_full_oof, sample_weight=y.map(test_weight_map))))\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ef99d1697b76c35619467f671da7024bba246087","trusted":true},"cell_type":"code","source":"pd.DataFrame(results['xt400_test'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3a7e17577d4bcef9a362827b1ab61da67ffd81f"},"cell_type":"markdown","source":"# Save submission"},{"metadata":{"_uuid":"084bba7999829ad9993605f382297e62789b3796","trusted":true},"cell_type":"code","source":"!ls ../input/","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf6738bc5a590342d85f21febd5e068d5081ce93","trusted":true},"cell_type":"code","source":"sub = pd.read_csv(PATH + 'sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd1e7601392789486d696bf33586309d6a53931d","trusted":true},"cell_type":"code","source":"for name,clf in clfs.items():\n    print('------ {} -----------'.format(name))\n    sub['Cover_Type'] = clf.predict(test_x) + 1\n    sub.to_csv('submission_{}.csv'.format(name), index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"795ea36b14dfeb796eb205919ed51d0a2c8086ff","trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"116103d1d83c4c7befe1890b31bc9aa2b3f7e40e"},"cell_type":"markdown","source":"Preditions for individual voting classifiers"},{"metadata":{"_uuid":"673bfb14a03b629d977ddded00826eb425d1b2bb","trusted":true},"cell_type":"code","source":"for name,x in vcs.items():\n    print('------ {} -----------'.format(name))\n    clf = x['vc_w0']\n    sub['Cover_Type'] = clf.predict(test_x) + 1\n    sub.to_csv('submission_voting_{}.csv'.format(name), index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c53ba442339d51063603b014d22e9939d4940fff","trusted":false},"cell_type":"markdown","source":"Predictions for the global voting"},{"metadata":{"trusted":true,"_uuid":"72531f6a909ad9b3f2f8317afc7385d6bc67fd27"},"cell_type":"code","source":"vc_global_1 = VotingPrefitClassifier([(n, vcs[n]['vc_w0']) for n in vcs.keys()]).fit(train_x, y)\nl = []\nfor n in vcs.keys():\n    l = l + vcs[n]['vc_w0'].estimators\nvc_global_2 = VotingPrefitClassifier(l).fit(train_x, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b2d128fe0408929bba9c88cf9f7c7ccded513e3d"},"cell_type":"code","source":"for vote_type in ['hard']:\n    vc_global_1.voting = vote_type\n    vc_global_2.voting = vote_type\n    sub['Cover_Type'] = vc_global_1.predict(test_x) + 1\n    sub.to_csv('submission_voting_global_1_{}.csv'.format(vote_type), index=False)\n    sub['Cover_Type'] = vc_global_2.predict(test_x) + 1\n    sub.to_csv('submission_voting_global_2_{}.csv'.format(vote_type), index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d660713c4e06f7aa71b4344dcba53c790af8a08"},"cell_type":"code","source":"vc_g","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}